{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the libraries \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D, Flatten, Dropout, concatenate,GlobalAveragePooling2D, AveragePooling2D, ZeroPadding2D, add, Add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform, Constant\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Activation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from numpy import argmax, array_equal\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/fashion/fashion-mnist_train.csv\")\n",
    "test = pd.read_csv(\"data/fashion/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[list(train.columns)[1:]].values\n",
    "train_y = train['label'].values\n",
    "test_x = test[list(test.columns)[1:]].values\n",
    "test_y = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.reshape(60000,28,28,1)\n",
    "test_x=test_x.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, num_classes = 10)\n",
    "test_y = to_categorical(test_y, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 6, \n",
    "                 kernel_size = 5, \n",
    "                 strides = 1, \n",
    "                 activation = 'relu', \n",
    "                 input_shape = (28,28,1)))\n",
    "model.add(MaxPool2D(pool_size = 2, strides = 2))\n",
    "model.add(Conv2D(filters = 16, \n",
    "                 kernel_size = 5,\n",
    "                 strides = 1,\n",
    "                 activation = 'relu',\n",
    "                 input_shape = (12,12,6)))\n",
    "model.add(MaxPool2D(pool_size = 2, strides = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 120, activation = 'relu'))\n",
    "model.add(Dense(units = 84, activation = 'relu'))\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 12s 258us/step - loss: 0.6155 - acc: 0.7744 - val_loss: 0.4250 - val_acc: 0.8454\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.4221 - acc: 0.8452 - val_loss: 0.3948 - val_acc: 0.8566\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.3710 - acc: 0.8634 - val_loss: 0.3525 - val_acc: 0.8698\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.3366 - acc: 0.8749 - val_loss: 0.3241 - val_acc: 0.8799\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.3102 - acc: 0.8852 - val_loss: 0.3113 - val_acc: 0.8881\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.2910 - acc: 0.8941 - val_loss: 0.2994 - val_acc: 0.8887\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2728 - acc: 0.8992 - val_loss: 0.2959 - val_acc: 0.8920\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 0.2587 - acc: 0.9025 - val_loss: 0.2971 - val_acc: 0.8931\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2460 - acc: 0.9084 - val_loss: 0.2886 - val_acc: 0.8992\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2334 - acc: 0.9138 - val_loss: 0.2937 - val_acc: 0.8967\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.2192 - acc: 0.9185 - val_loss: 0.2934 - val_acc: 0.9003\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.2110 - acc: 0.9209 - val_loss: 0.3014 - val_acc: 0.8943\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2032 - acc: 0.9240 - val_loss: 0.2978 - val_acc: 0.9016\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1916 - acc: 0.9272 - val_loss: 0.2978 - val_acc: 0.8985\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.1817 - acc: 0.9306 - val_loss: 0.2994 - val_acc: 0.9032\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1752 - acc: 0.9339 - val_loss: 0.3080 - val_acc: 0.9006\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.1680 - acc: 0.9371 - val_loss: 0.3110 - val_acc: 0.8992\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.1609 - acc: 0.9391 - val_loss: 0.3150 - val_acc: 0.9049\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1530 - acc: 0.9428 - val_loss: 0.3409 - val_acc: 0.8975\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1477 - acc: 0.9434 - val_loss: 0.3161 - val_acc: 0.9034\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1413 - acc: 0.9467 - val_loss: 0.3503 - val_acc: 0.8966\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.1357 - acc: 0.9490 - val_loss: 0.3351 - val_acc: 0.9034\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.1294 - acc: 0.9506 - val_loss: 0.3628 - val_acc: 0.8981\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.1238 - acc: 0.9530 - val_loss: 0.3836 - val_acc: 0.8918\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1173 - acc: 0.9550 - val_loss: 0.3949 - val_acc: 0.8988\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1164 - acc: 0.9564 - val_loss: 0.3774 - val_acc: 0.8992\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1099 - acc: 0.9579 - val_loss: 0.3992 - val_acc: 0.8991\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1053 - acc: 0.9594 - val_loss: 0.4194 - val_acc: 0.8919\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.1057 - acc: 0.9599 - val_loss: 0.4204 - val_acc: 0.8955\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.0995 - acc: 0.9618 - val_loss: 0.4227 - val_acc: 0.8974\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0923 - acc: 0.9646 - val_loss: 0.4621 - val_acc: 0.8998\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0944 - acc: 0.9633 - val_loss: 0.4867 - val_acc: 0.8940\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.0873 - acc: 0.9670 - val_loss: 0.4708 - val_acc: 0.8963\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0830 - acc: 0.9679 - val_loss: 0.4940 - val_acc: 0.8943\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.0828 - acc: 0.9675 - val_loss: 0.4869 - val_acc: 0.8975\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.0834 - acc: 0.9680 - val_loss: 0.5074 - val_acc: 0.8929\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0808 - acc: 0.9684 - val_loss: 0.4950 - val_acc: 0.8944\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.0767 - acc: 0.9711 - val_loss: 0.4963 - val_acc: 0.8934\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0696 - acc: 0.9730 - val_loss: 0.5159 - val_acc: 0.8953\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0734 - acc: 0.9728 - val_loss: 0.5850 - val_acc: 0.8848\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.0707 - acc: 0.9734 - val_loss: 0.5450 - val_acc: 0.8954\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.0697 - acc: 0.9733 - val_loss: 0.5715 - val_acc: 0.8924\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0643 - acc: 0.9762 - val_loss: 0.5456 - val_acc: 0.8958\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.0679 - acc: 0.9754 - val_loss: 0.5604 - val_acc: 0.8953\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0598 - acc: 0.9773 - val_loss: 0.5770 - val_acc: 0.8933\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.0612 - acc: 0.9773 - val_loss: 0.6071 - val_acc: 0.8898\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0615 - acc: 0.9769 - val_loss: 0.6259 - val_acc: 0.8906\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.0590 - acc: 0.9779 - val_loss: 0.5887 - val_acc: 0.8912\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 6s 130us/step - loss: 0.0607 - acc: 0.9771 - val_loss: 0.6300 - val_acc: 0.8931\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.0577 - acc: 0.9783 - val_loss: 0.6463 - val_acc: 0.8891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2066c96aa58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8887"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(input_shape=(28,28,1), filters=96, kernel_size = 11, strides = 1, padding='same', \n",
    "                 activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size = 5, strides = 1, padding='same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=1024, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=1024, kernel_size = 3, strides = 1, padding='same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 3072, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(units = 4096, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(units = 10, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 96)        11712     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 3, 1024)        9438208   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3072)              3148800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              12587008  \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 31,781,450\n",
      "Trainable params: 31,761,290\n",
      "Non-trainable params: 20,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 130s 3ms/step - loss: 0.8755 - acc: 0.7525 - val_loss: 0.4721 - val_acc: 0.8490\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.5626 - acc: 0.8298 - val_loss: 0.4765 - val_acc: 0.8399\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.5723 - acc: 0.8406 - val_loss: 0.7166 - val_acc: 0.7894\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.5528 - acc: 0.8430 - val_loss: 0.5709 - val_acc: 0.8272\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4303 - acc: 0.8681 - val_loss: 0.3706 - val_acc: 0.8814\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4528 - acc: 0.8680 - val_loss: 0.3979 - val_acc: 0.8776\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4387 - acc: 0.8762 - val_loss: 0.4603 - val_acc: 0.8509\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.5311 - acc: 0.8573 - val_loss: 0.3651 - val_acc: 0.8911\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4169 - acc: 0.8784 - val_loss: 0.3841 - val_acc: 0.8800\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3957 - acc: 0.8832 - val_loss: 0.4369 - val_acc: 0.8616\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4463 - acc: 0.8789 - val_loss: 0.3546 - val_acc: 0.8873\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4181 - acc: 0.8805 - val_loss: 1.0678 - val_acc: 0.8438\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.4177 - acc: 0.8816 - val_loss: 0.3544 - val_acc: 0.8933\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3720 - acc: 0.8920 - val_loss: 0.3234 - val_acc: 0.8956\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3431 - acc: 0.8957 - val_loss: 0.7685 - val_acc: 0.8348\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3423 - acc: 0.8993 - val_loss: 0.4325 - val_acc: 0.8795\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3749 - acc: 0.8956 - val_loss: 0.6422 - val_acc: 0.7890\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3431 - acc: 0.9009 - val_loss: 0.3209 - val_acc: 0.8987\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2959 - acc: 0.9088 - val_loss: 0.3125 - val_acc: 0.9011\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3330 - acc: 0.9005 - val_loss: 0.3055 - val_acc: 0.9040\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3124 - acc: 0.9099 - val_loss: 0.3278 - val_acc: 0.9047\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.6210 - acc: 0.8510 - val_loss: 0.3937 - val_acc: 0.8876\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3844 - acc: 0.8895 - val_loss: 0.4425 - val_acc: 0.8970\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3270 - acc: 0.9068 - val_loss: 0.3449 - val_acc: 0.8977\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2741 - acc: 0.9210 - val_loss: 0.3279 - val_acc: 0.8932\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2819 - acc: 0.9148 - val_loss: 0.5437 - val_acc: 0.8902\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3144 - acc: 0.9125 - val_loss: 0.3015 - val_acc: 0.9111\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2867 - acc: 0.9221 - val_loss: 0.4274 - val_acc: 0.8980\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3630 - acc: 0.9048 - val_loss: 0.3887 - val_acc: 0.8812\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.3090 - acc: 0.9141 - val_loss: 0.4891 - val_acc: 0.9031\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2531 - acc: 0.9264 - val_loss: 0.3092 - val_acc: 0.9044\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2433 - acc: 0.9274 - val_loss: 0.4131 - val_acc: 0.8944\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2153 - acc: 0.9358 - val_loss: 0.8017 - val_acc: 0.8482\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2727 - acc: 0.9189 - val_loss: 0.6339 - val_acc: 0.8890\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.2106 - acc: 0.9376 - val_loss: 0.4043 - val_acc: 0.9053\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 128s 3ms/step - loss: 0.1837 - acc: 0.9428 - val_loss: 0.3951 - val_acc: 0.8952\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1774 - acc: 0.9443 - val_loss: 0.3513 - val_acc: 0.9103\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 128s 3ms/step - loss: 0.1590 - acc: 0.9500 - val_loss: 0.3474 - val_acc: 0.8897\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1909 - acc: 0.9403 - val_loss: 0.3706 - val_acc: 0.9094\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1538 - acc: 0.9529 - val_loss: 0.4908 - val_acc: 0.8876\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1449 - acc: 0.9556 - val_loss: 0.3877 - val_acc: 0.9050\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1385 - acc: 0.9579 - val_loss: 0.4551 - val_acc: 0.8984\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1346 - acc: 0.9620 - val_loss: 0.3768 - val_acc: 0.9085\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1299 - acc: 0.9633 - val_loss: 0.4686 - val_acc: 0.9028\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1357 - acc: 0.9609 - val_loss: 0.4078 - val_acc: 0.8964\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1055 - acc: 0.9709 - val_loss: 0.4022 - val_acc: 0.9108\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1109 - acc: 0.9689 - val_loss: 0.5370 - val_acc: 0.9003\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1167 - acc: 0.9674 - val_loss: 0.3947 - val_acc: 0.8972\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1551 - acc: 0.9578 - val_loss: 0.3700 - val_acc: 0.9079\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 127s 3ms/step - loss: 0.1120 - acc: 0.9710 - val_loss: 0.3950 - val_acc: 0.9115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20621e6d1d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(input_shape=(28,28,1), filters=96, kernel_size=7, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(MaxPool2D(pool_size=2,strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=512,kernel_size=3,strides=1,padding=\"same\", kernel_initializer=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=1024, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"uniform\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(MaxPool2D(pool_size=2,strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(units=4096, activation=\"relu\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_161 (Conv2D)          (None, 14, 14, 96)        4800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 7, 7, 96)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 96)          384       \n",
      "_________________________________________________________________\n",
      "conv2d_162 (Conv2D)          (None, 4, 4, 256)         614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_163 (Conv2D)          (None, 2, 2, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_164 (Conv2D)          (None, 2, 2, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv2d_165 (Conv2D)          (None, 2, 2, 512)         4719104   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4096)              2101248   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 30,171,466\n",
      "Trainable params: 30,166,666\n",
      "Non-trainable params: 4,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 98s 2ms/step - loss: 0.8308 - acc: 0.7569 - val_loss: 0.5012 - val_acc: 0.8339\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.6562 - acc: 0.8163 - val_loss: 0.3994 - val_acc: 0.8782\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.8821 - acc: 0.7980 - val_loss: 0.5899 - val_acc: 0.7980\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.5822 - acc: 0.8306 - val_loss: 0.3867 - val_acc: 0.8638\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.5068 - acc: 0.8567 - val_loss: 0.3882 - val_acc: 0.8757\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.5339 - acc: 0.8524 - val_loss: 0.3701 - val_acc: 0.8801\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.4808 - acc: 0.8586 - val_loss: 0.3730 - val_acc: 0.8768\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.4507 - acc: 0.8704 - val_loss: 0.3447 - val_acc: 0.8915\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.4230 - acc: 0.8726 - val_loss: 0.3849 - val_acc: 0.8858\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.4019 - acc: 0.8810 - val_loss: 0.3492 - val_acc: 0.8947\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.3539 - acc: 0.8905 - val_loss: 0.3863 - val_acc: 0.8654\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.3915 - acc: 0.8822 - val_loss: 0.4117 - val_acc: 0.8822\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.3283 - acc: 0.8951 - val_loss: 0.3487 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.3061 - acc: 0.9008 - val_loss: 0.3373 - val_acc: 0.8922\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.3014 - acc: 0.9039 - val_loss: 0.3298 - val_acc: 0.9042\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.2742 - acc: 0.9103 - val_loss: 0.3099 - val_acc: 0.9080\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.2628 - acc: 0.9141 - val_loss: 0.3076 - val_acc: 0.9115\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.2526 - acc: 0.9184 - val_loss: 0.2919 - val_acc: 0.9075\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.2385 - acc: 0.9233 - val_loss: 0.2601 - val_acc: 0.9129\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.2245 - acc: 0.9253 - val_loss: 0.2920 - val_acc: 0.9080\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.2158 - acc: 0.9279 - val_loss: 0.2911 - val_acc: 0.9004\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1988 - acc: 0.9349 - val_loss: 0.3347 - val_acc: 0.9040\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1953 - acc: 0.9361 - val_loss: 0.2929 - val_acc: 0.9051\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1785 - acc: 0.9420 - val_loss: 0.2910 - val_acc: 0.9071\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1731 - acc: 0.9443 - val_loss: 0.2762 - val_acc: 0.9167\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1660 - acc: 0.9472 - val_loss: 0.2887 - val_acc: 0.9089\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1527 - acc: 0.9508 - val_loss: 0.2935 - val_acc: 0.9130\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1486 - acc: 0.9537 - val_loss: 0.2901 - val_acc: 0.9112\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1472 - acc: 0.9536 - val_loss: 0.2974 - val_acc: 0.9093\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1404 - acc: 0.9552 - val_loss: 0.3267 - val_acc: 0.8997\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1251 - acc: 0.9610 - val_loss: 0.3451 - val_acc: 0.9113\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1261 - acc: 0.9611 - val_loss: 0.3268 - val_acc: 0.9121\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1186 - acc: 0.9641 - val_loss: 0.3559 - val_acc: 0.8963\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1168 - acc: 0.9643 - val_loss: 0.3458 - val_acc: 0.9082\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1066 - acc: 0.9671 - val_loss: 0.3671 - val_acc: 0.8860\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1085 - acc: 0.9668 - val_loss: 0.3652 - val_acc: 0.9066\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1024 - acc: 0.9690 - val_loss: 0.3460 - val_acc: 0.9160\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.1235 - acc: 0.9673 - val_loss: 0.3507 - val_acc: 0.9012\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0992 - acc: 0.9700 - val_loss: 0.4270 - val_acc: 0.9096\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0954 - acc: 0.9710 - val_loss: 0.3715 - val_acc: 0.9151\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0911 - acc: 0.9723 - val_loss: 0.3325 - val_acc: 0.9137\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0986 - acc: 0.9716 - val_loss: 0.3311 - val_acc: 0.9135\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0831 - acc: 0.9761 - val_loss: 0.3668 - val_acc: 0.9080\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0878 - acc: 0.9752 - val_loss: 0.3876 - val_acc: 0.9105\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0824 - acc: 0.9757 - val_loss: 0.4507 - val_acc: 0.8959 acc: 0\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0789 - acc: 0.9775 - val_loss: 0.4009 - val_acc: 0.9015\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0796 - acc: 0.9775 - val_loss: 0.3805 - val_acc: 0.9092\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0779 - acc: 0.9787 - val_loss: 0.3848 - val_acc: 0.9080\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0723 - acc: 0.9795 - val_loss: 0.4999 - val_acc: 0.8967\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 94s 2ms/step - loss: 0.0796 - acc: 0.9781 - val_loss: 0.4048 - val_acc: 0.9142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207ed050048>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj, name=None):\n",
    "    \n",
    "    #padding=same is mandatory,initializer is always glorot_uniform and uniform init=0.2 conv_1x1 is now a mini-model\n",
    "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
    "    \n",
    "    #conv_3x3 is now a mini model of x\n",
    "    conv_3x3 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
    "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_3x3)\n",
    "\n",
    "    #conv_5x5 is now a mini model of x\n",
    "    conv_5x5 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(x)\n",
    "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(conv_5x5)\n",
    "\n",
    "    #pool_proj is now a mini model of x\n",
    "    pool_proj = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer=bias_init)(pool_proj)\n",
    "\n",
    "    #We can concatenate those \"mini-models\" as they give an output with same width-height\n",
    "    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = glorot_uniform()\n",
    "bias_init = Constant(value=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(28, 28, 1))\n",
    "\n",
    "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2', kernel_initializer=kernel_init, bias_initializer=bias_init)(input_layer)\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3/2')(x)\n",
    "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3/1')(x)\n",
    "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3/1')(x)\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3/2')(x)\n",
    "\n",
    "x = inception_module(x,filters_1x1=64,filters_3x3_reduce=96,filters_3x3=128,filters_5x5_reduce=16,filters_5x5=32,\n",
    "                     filters_pool_proj=32,name='inception_3a')\n",
    "\n",
    "x = inception_module(x,filters_1x1=128,filters_3x3_reduce=128,filters_3x3=192,filters_5x5_reduce=32,filters_5x5=96,\n",
    "                     filters_pool_proj=64,name='inception_3b')\n",
    "\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3/2')(x)\n",
    "\n",
    "x = inception_module(x,filters_1x1=192,filters_3x3_reduce=96,filters_3x3=208,filters_5x5_reduce=16,filters_5x5=48,\n",
    "                     filters_pool_proj=64,name='inception_4a')\n",
    "\n",
    "#x1 = AveragePooling2D((5, 5), strides=3)(x)\n",
    "x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x)\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(1024, activation='relu')(x1)\n",
    "x1 = Dropout(rate=0.3)(x1)\n",
    "x1 = Dense(10, activation='softmax', name='auxilliary_output_1')(x1)\n",
    "\n",
    "x = inception_module(x,filters_1x1=160,filters_3x3_reduce=112,filters_3x3=224,filters_5x5_reduce=24,filters_5x5=64,\n",
    "                     filters_pool_proj=64,name='inception_4b')\n",
    "\n",
    "x = inception_module(x,filters_1x1=128,filters_3x3_reduce=128,filters_3x3=256,filters_5x5_reduce=24,filters_5x5=64,\n",
    "                     filters_pool_proj=64,name='inception_4c')\n",
    "\n",
    "x = inception_module(x,filters_1x1=112,filters_3x3_reduce=144,filters_3x3=288,filters_5x5_reduce=32,filters_5x5=64,\n",
    "                     filters_pool_proj=64,name='inception_4d')\n",
    "\n",
    "#x2 = AveragePooling2D((5, 5), strides=3)(x)\n",
    "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(1024, activation='relu')(x2)\n",
    "x2 = Dropout(rate=0.3)(x2)\n",
    "x2 = Dense(10, activation='softmax', name='auxilliary_output_2')(x2)\n",
    "\n",
    "x = inception_module(x,filters_1x1=256,filters_3x3_reduce=160,filters_3x3=320,filters_5x5_reduce=32,filters_5x5=128,\n",
    "                     filters_pool_proj=128,name='inception_4e')\n",
    "\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3/2')(x)\n",
    "\n",
    "x = inception_module(x,filters_1x1=256,filters_3x3_reduce=160,filters_3x3=320,filters_5x5_reduce=32,filters_5x5=128,\n",
    "                     filters_pool_proj=128,name='inception_5a')\n",
    "\n",
    "x = inception_module(x,filters_1x1=384,filters_3x3_reduce=192,filters_3x3=384,filters_5x5_reduce=48,filters_5x5=128,\n",
    "                     filters_pool_proj=128,name='inception_5b')\n",
    "\n",
    "#Attach the \"main\" ending\n",
    "x = GlobalAveragePooling2D(name='avg_pool_5_3x3/1')(x)\n",
    "\n",
    "x = Dropout(rate=0.6)(x)\n",
    "\n",
    "x = Dense(10, activation='softmax', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_layer],outputs=[x, x1, x2], name='inception_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_7x7/2 (Conv2D)           (None, 14, 14, 64)   3200        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_1_3x3/2 (MaxPooling2D) (None, 7, 7, 64)     0           conv_1_7x7/2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2a_3x3/1 (Conv2D)          (None, 7, 7, 64)     4160        max_pool_1_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_2b_3x3/1 (Conv2D)          (None, 7, 7, 192)    110784      conv_2a_3x3/1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_2_3x3/2 (MaxPooling2D) (None, 4, 4, 192)    0           conv_2b_3x3/1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 4, 4, 96)     18528       max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 4, 4, 16)     3088        max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 4, 4, 192)    0           max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 4, 4, 64)     12352       max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 4, 4, 128)    110720      conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 4, 4, 32)     12832       conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 4, 4, 32)     6176        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3a (Concatenate)      (None, 4, 4, 256)    0           conv2d_64[0][0]                  \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "                                                                 conv2d_68[0][0]                  \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 4, 4, 128)    32896       inception_3a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 4, 4, 32)     8224        inception_3a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D) (None, 4, 4, 256)    0           inception_3a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 4, 4, 128)    32896       inception_3a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 4, 4, 192)    221376      conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 4, 4, 96)     76896       conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 4, 4, 64)     16448       max_pooling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_3b (Concatenate)      (None, 4, 4, 480)    0           conv2d_70[0][0]                  \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "                                                                 conv2d_74[0][0]                  \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_3_3x3/2 (MaxPooling2D) (None, 2, 2, 480)    0           inception_3b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 2, 2, 96)     46176       max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 2, 2, 16)     7696        max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D) (None, 2, 2, 480)    0           max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 2, 2, 192)    92352       max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 2, 2, 208)    179920      conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 2, 2, 48)     19248       conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 2, 2, 64)     30784       max_pooling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4a (Concatenate)      (None, 2, 2, 512)    0           conv2d_76[0][0]                  \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "                                                                 conv2d_80[0][0]                  \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 2, 2, 112)    57456       inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 2, 2, 24)     12312       inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 2, 2, 512)    0           inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 2, 2, 160)    82080       inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 2, 2, 224)    226016      conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 2, 2, 64)     38464       conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 2, 2, 64)     32832       max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4b (Concatenate)      (None, 2, 2, 512)    0           conv2d_83[0][0]                  \n",
      "                                                                 conv2d_85[0][0]                  \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "                                                                 conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 2, 2, 128)    65664       inception_4b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 2, 2, 24)     12312       inception_4b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 2, 2, 512)    0           inception_4b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 2, 2, 128)    65664       inception_4b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 2, 2, 256)    295168      conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 2, 2, 64)     38464       conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 2, 2, 64)     32832       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4c (Concatenate)      (None, 2, 2, 512)    0           conv2d_89[0][0]                  \n",
      "                                                                 conv2d_91[0][0]                  \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "                                                                 conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 2, 2, 144)    73872       inception_4c[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 2, 2, 32)     16416       inception_4c[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 2, 2, 512)    0           inception_4c[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 2, 2, 112)    57456       inception_4c[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 2, 2, 288)    373536      conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 2, 2, 64)     51264       conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 2, 2, 64)     32832       max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4d (Concatenate)      (None, 2, 2, 528)    0           conv2d_95[0][0]                  \n",
      "                                                                 conv2d_97[0][0]                  \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "                                                                 conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 2, 2, 160)    84640       inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 2, 2, 32)     16928       inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 2, 2, 528)    0           inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 2, 2, 256)    135424      inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 2, 2, 320)    461120      conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 2, 2, 128)    102528      conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 2, 2, 128)    67712       max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_4e (Concatenate)      (None, 2, 2, 832)    0           conv2d_102[0][0]                 \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "                                                                 conv2d_106[0][0]                 \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_4_3x3/2 (MaxPooling2D) (None, 1, 1, 832)    0           inception_4e[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 1, 1, 160)    133280      max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 1, 1, 32)     26656       max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 1, 1, 832)    0           max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 1, 1, 256)    213248      max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 1, 1, 320)    461120      conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 1, 1, 128)    102528      conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 1, 1, 128)    106624      max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "inception_5a (Concatenate)      (None, 1, 1, 832)    0           conv2d_108[0][0]                 \n",
      "                                                                 conv2d_110[0][0]                 \n",
      "                                                                 conv2d_112[0][0]                 \n",
      "                                                                 conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 1, 1, 192)    159936      inception_5a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 1, 1, 48)     39984       inception_5a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 1, 1, 832)    0           inception_5a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 1, 1, 384)    319872      inception_5a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 1, 1, 384)    663936      conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 1, 1, 128)    153728      conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 1, 1, 128)    106624      max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 2, 2, 128)    65664       inception_4a[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 2, 2, 128)    67712       inception_4d[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "inception_5b (Concatenate)      (None, 1, 1, 1024)   0           conv2d_114[0][0]                 \n",
      "                                                                 conv2d_116[0][0]                 \n",
      "                                                                 conv2d_118[0][0]                 \n",
      "                                                                 conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 512)          0           conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 512)          0           conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_5_3x3/1 (GlobalAverage (None, 1024)         0           inception_5b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         525312      flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1024)         525312      flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1024)         0           avg_pool_5_3x3/1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1024)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 10)           10250       dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "auxilliary_output_1 (Dense)     (None, 10)           10250       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "auxilliary_output_2 (Dense)     (None, 10)           10250       dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,182,030\n",
      "Trainable params: 7,182,030\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 1.3708 - output_loss: 0.8938 - auxilliary_output_1_loss: 0.7571 - auxilliary_output_2_loss: 0.8330 - output_acc: 0.6598 - auxilliary_output_1_acc: 0.7076 - auxilliary_output_2_acc: 0.6766 - val_loss: 0.7732 - val_output_loss: 0.5033 - val_auxilliary_output_1_loss: 0.4331 - val_auxilliary_output_2_loss: 0.4663 - val_output_acc: 0.8348 - val_auxilliary_output_1_acc: 0.8419 - val_auxilliary_output_2_acc: 0.8399\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.7510 - output_loss: 0.4857 - auxilliary_output_1_loss: 0.4337 - auxilliary_output_2_loss: 0.4508 - output_acc: 0.8301 - auxilliary_output_1_acc: 0.8421 - auxilliary_output_2_acc: 0.8380 - val_loss: 0.6146 - val_output_loss: 0.3961 - val_auxilliary_output_1_loss: 0.3586 - val_auxilliary_output_2_loss: 0.3699 - val_output_acc: 0.8593 - val_auxilliary_output_1_acc: 0.8655 - val_auxilliary_output_2_acc: 0.8627\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.6200 - output_loss: 0.3991 - auxilliary_output_1_loss: 0.3633 - auxilliary_output_2_loss: 0.3731 - output_acc: 0.8579 - auxilliary_output_1_acc: 0.8676 - auxilliary_output_2_acc: 0.8644 - val_loss: 0.5369 - val_output_loss: 0.3385 - val_auxilliary_output_1_loss: 0.3312 - val_auxilliary_output_2_loss: 0.3301 - val_output_acc: 0.8845 - val_auxilliary_output_1_acc: 0.8820 - val_auxilliary_output_2_acc: 0.8837\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.5572 - output_loss: 0.3570 - auxilliary_output_1_loss: 0.3294 - auxilliary_output_2_loss: 0.3379 - output_acc: 0.8732 - auxilliary_output_1_acc: 0.8798 - auxilliary_output_2_acc: 0.8774 - val_loss: 0.5402 - val_output_loss: 0.3436 - val_auxilliary_output_1_loss: 0.3269 - val_auxilliary_output_2_loss: 0.3284 - val_output_acc: 0.8746 - val_auxilliary_output_1_acc: 0.8793 - val_auxilliary_output_2_acc: 0.8818\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.5195 - output_loss: 0.3329 - auxilliary_output_1_loss: 0.3095 - auxilliary_output_2_loss: 0.3125 - output_acc: 0.8821 - auxilliary_output_1_acc: 0.8879 - auxilliary_output_2_acc: 0.8873 - val_loss: 0.5106 - val_output_loss: 0.3265 - val_auxilliary_output_1_loss: 0.3009 - val_auxilliary_output_2_loss: 0.3125 - val_output_acc: 0.8872 - val_auxilliary_output_1_acc: 0.8920 - val_auxilliary_output_2_acc: 0.8900\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.4865 - output_loss: 0.3118 - auxilliary_output_1_loss: 0.2897 - auxilliary_output_2_loss: 0.2927 - output_acc: 0.8920 - auxilliary_output_1_acc: 0.8964 - auxilliary_output_2_acc: 0.8966 - val_loss: 0.4947 - val_output_loss: 0.3149 - val_auxilliary_output_1_loss: 0.2966 - val_auxilliary_output_2_loss: 0.3027 - val_output_acc: 0.8928 - val_auxilliary_output_1_acc: 0.8956 - val_auxilliary_output_2_acc: 0.8961\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.4673 - output_loss: 0.2981 - auxilliary_output_1_loss: 0.2820 - auxilliary_output_2_loss: 0.2822 - output_acc: 0.8948 - auxilliary_output_1_acc: 0.8974 - auxilliary_output_2_acc: 0.8986 - val_loss: 0.4922 - val_output_loss: 0.3119 - val_auxilliary_output_1_loss: 0.2955 - val_auxilliary_output_2_loss: 0.3055 - val_output_acc: 0.8866 - val_auxilliary_output_1_acc: 0.8919 - val_auxilliary_output_2_acc: 0.8904\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.4390 - output_loss: 0.2803 - auxilliary_output_1_loss: 0.2650 - auxilliary_output_2_loss: 0.2642 - output_acc: 0.9012 - auxilliary_output_1_acc: 0.9046 - auxilliary_output_2_acc: 0.9048 - val_loss: 0.4568 - val_output_loss: 0.2877 - val_auxilliary_output_1_loss: 0.2827 - val_auxilliary_output_2_loss: 0.2809 - val_output_acc: 0.8967 - val_auxilliary_output_1_acc: 0.9008 - val_auxilliary_output_2_acc: 0.8998\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.4243 - output_loss: 0.2709 - auxilliary_output_1_loss: 0.2552 - auxilliary_output_2_loss: 0.2564 - output_acc: 0.9036 - auxilliary_output_1_acc: 0.9064 - auxilliary_output_2_acc: 0.9068 - val_loss: 0.4831 - val_output_loss: 0.3080 - val_auxilliary_output_1_loss: 0.2900 - val_auxilliary_output_2_loss: 0.2938 - val_output_acc: 0.8943 - val_auxilliary_output_1_acc: 0.8985 - val_auxilliary_output_2_acc: 0.8976\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.4041 - output_loss: 0.2570 - auxilliary_output_1_loss: 0.2462 - auxilliary_output_2_loss: 0.2441 - output_acc: 0.9098 - auxilliary_output_1_acc: 0.9117 - auxilliary_output_2_acc: 0.9123 - val_loss: 0.4497 - val_output_loss: 0.2836 - val_auxilliary_output_1_loss: 0.2775 - val_auxilliary_output_2_loss: 0.2761 - val_output_acc: 0.8997 - val_auxilliary_output_1_acc: 0.9024 - val_auxilliary_output_2_acc: 0.9024\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3821 - output_loss: 0.2430 - auxilliary_output_1_loss: 0.2337 - auxilliary_output_2_loss: 0.2299 - output_acc: 0.9129 - auxilliary_output_1_acc: 0.9146 - auxilliary_output_2_acc: 0.9160 - val_loss: 0.4405 - val_output_loss: 0.2783 - val_auxilliary_output_1_loss: 0.2713 - val_auxilliary_output_2_loss: 0.2694 - val_output_acc: 0.9086 - val_auxilliary_output_1_acc: 0.9087 - val_auxilliary_output_2_acc: 0.9072\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3822 - output_loss: 0.2420 - auxilliary_output_1_loss: 0.2328 - auxilliary_output_2_loss: 0.2347 - output_acc: 0.9143 - auxilliary_output_1_acc: 0.9163 - auxilliary_output_2_acc: 0.9171 - val_loss: 0.4281 - val_output_loss: 0.2692 - val_auxilliary_output_1_loss: 0.2657 - val_auxilliary_output_2_loss: 0.2640 - val_output_acc: 0.9065 - val_auxilliary_output_1_acc: 0.9087 - val_auxilliary_output_2_acc: 0.9072\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3609 - output_loss: 0.2279 - auxilliary_output_1_loss: 0.2218 - auxilliary_output_2_loss: 0.2215 - output_acc: 0.9185 - auxilliary_output_1_acc: 0.9201 - auxilliary_output_2_acc: 0.9205 - val_loss: 0.4423 - val_output_loss: 0.2791 - val_auxilliary_output_1_loss: 0.2716 - val_auxilliary_output_2_loss: 0.2725 - val_output_acc: 0.8987 - val_auxilliary_output_1_acc: 0.9021 - val_auxilliary_output_2_acc: 0.9007\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3416 - output_loss: 0.2170 - auxilliary_output_1_loss: 0.2098 - auxilliary_output_2_loss: 0.2055 - output_acc: 0.9232 - auxilliary_output_1_acc: 0.9235 - auxilliary_output_2_acc: 0.9245 - val_loss: 0.4542 - val_output_loss: 0.2854 - val_auxilliary_output_1_loss: 0.2889 - val_auxilliary_output_2_loss: 0.2739 - val_output_acc: 0.9010 - val_auxilliary_output_1_acc: 0.9005 - val_auxilliary_output_2_acc: 0.9037\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3397 - output_loss: 0.2155 - auxilliary_output_1_loss: 0.2095 - auxilliary_output_2_loss: 0.2044 - output_acc: 0.9235 - auxilliary_output_1_acc: 0.9247 - auxilliary_output_2_acc: 0.9259 - val_loss: 0.4537 - val_output_loss: 0.2860 - val_auxilliary_output_1_loss: 0.2773 - val_auxilliary_output_2_loss: 0.2814 - val_output_acc: 0.9018 - val_auxilliary_output_1_acc: 0.9077 - val_auxilliary_output_2_acc: 0.9017\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3202 - output_loss: 0.2028 - auxilliary_output_1_loss: 0.1983 - auxilliary_output_2_loss: 0.1930 - output_acc: 0.9266 - auxilliary_output_1_acc: 0.9289 - auxilliary_output_2_acc: 0.9292 - val_loss: 0.4638 - val_output_loss: 0.2972 - val_auxilliary_output_1_loss: 0.2770 - val_auxilliary_output_2_loss: 0.2783 - val_output_acc: 0.9046 - val_auxilliary_output_1_acc: 0.9066 - val_auxilliary_output_2_acc: 0.9061\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3113 - output_loss: 0.1979 - auxilliary_output_1_loss: 0.1912 - auxilliary_output_2_loss: 0.1870 - output_acc: 0.9300 - auxilliary_output_1_acc: 0.9319 - auxilliary_output_2_acc: 0.9322 - val_loss: 0.4507 - val_output_loss: 0.2840 - val_auxilliary_output_1_loss: 0.2780 - val_auxilliary_output_2_loss: 0.2777 - val_output_acc: 0.9069 - val_auxilliary_output_1_acc: 0.9068 - val_auxilliary_output_2_acc: 0.9068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 67s 1ms/step - loss: 0.3026 - output_loss: 0.1917 - auxilliary_output_1_loss: 0.1877 - auxilliary_output_2_loss: 0.1818 - output_acc: 0.9316 - auxilliary_output_1_acc: 0.9320 - auxilliary_output_2_acc: 0.9332 - val_loss: 0.4539 - val_output_loss: 0.2827 - val_auxilliary_output_1_loss: 0.2912 - val_auxilliary_output_2_loss: 0.2796 - val_output_acc: 0.9017 - val_auxilliary_output_1_acc: 0.9037 - val_auxilliary_output_2_acc: 0.9056\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2916 - output_loss: 0.1843 - auxilliary_output_1_loss: 0.1834 - auxilliary_output_2_loss: 0.1741 - output_acc: 0.9335 - auxilliary_output_1_acc: 0.9337 - auxilliary_output_2_acc: 0.9358 - val_loss: 0.4701 - val_output_loss: 0.2951 - val_auxilliary_output_1_loss: 0.2952 - val_auxilliary_output_2_loss: 0.2880 - val_output_acc: 0.9028 - val_auxilliary_output_1_acc: 0.9032 - val_auxilliary_output_2_acc: 0.9018\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2769 - output_loss: 0.1759 - auxilliary_output_1_loss: 0.1713 - auxilliary_output_2_loss: 0.1655 - output_acc: 0.9378 - auxilliary_output_1_acc: 0.9375 - auxilliary_output_2_acc: 0.9390 - val_loss: 0.4721 - val_output_loss: 0.2910 - val_auxilliary_output_1_loss: 0.3168 - val_auxilliary_output_2_loss: 0.2870 - val_output_acc: 0.9060 - val_auxilliary_output_1_acc: 0.9058 - val_auxilliary_output_2_acc: 0.9073\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.3026 - output_loss: 0.1862 - auxilliary_output_1_loss: 0.1781 - auxilliary_output_2_loss: 0.2101 - output_acc: 0.9342 - auxilliary_output_1_acc: 0.9341 - auxilliary_output_2_acc: 0.9341 - val_loss: 0.6218 - val_output_loss: 0.3903 - val_auxilliary_output_1_loss: 0.4000 - val_auxilliary_output_2_loss: 0.3716 - val_output_acc: 0.8699 - val_auxilliary_output_1_acc: 0.8702 - val_auxilliary_output_2_acc: 0.8724\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2802 - output_loss: 0.1781 - auxilliary_output_1_loss: 0.1742 - auxilliary_output_2_loss: 0.1662 - output_acc: 0.9358 - auxilliary_output_1_acc: 0.9365 - auxilliary_output_2_acc: 0.9388 - val_loss: 0.4859 - val_output_loss: 0.3058 - val_auxilliary_output_1_loss: 0.3099 - val_auxilliary_output_2_loss: 0.2906 - val_output_acc: 0.9072 - val_auxilliary_output_1_acc: 0.9077 - val_auxilliary_output_2_acc: 0.9072\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2620 - output_loss: 0.1668 - auxilliary_output_1_loss: 0.1602 - auxilliary_output_2_loss: 0.1573 - output_acc: 0.9410 - auxilliary_output_1_acc: 0.9424 - auxilliary_output_2_acc: 0.9434 - val_loss: 0.4873 - val_output_loss: 0.3080 - val_auxilliary_output_1_loss: 0.3061 - val_auxilliary_output_2_loss: 0.2917 - val_output_acc: 0.9026 - val_auxilliary_output_1_acc: 0.9010 - val_auxilliary_output_2_acc: 0.9036\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2439 - output_loss: 0.1546 - auxilliary_output_1_loss: 0.1516 - auxilliary_output_2_loss: 0.1462 - output_acc: 0.9433 - auxilliary_output_1_acc: 0.9444 - auxilliary_output_2_acc: 0.9458 - val_loss: 0.5003 - val_output_loss: 0.3151 - val_auxilliary_output_1_loss: 0.3129 - val_auxilliary_output_2_loss: 0.3045 - val_output_acc: 0.9021 - val_auxilliary_output_1_acc: 0.9052 - val_auxilliary_output_2_acc: 0.9051\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2554 - output_loss: 0.1656 - auxilliary_output_1_loss: 0.1545 - auxilliary_output_2_loss: 0.1448 - output_acc: 0.9446 - auxilliary_output_1_acc: 0.9448 - auxilliary_output_2_acc: 0.9474 - val_loss: 0.5134 - val_output_loss: 0.3259 - val_auxilliary_output_1_loss: 0.3178 - val_auxilliary_output_2_loss: 0.3075 - val_output_acc: 0.9094 - val_auxilliary_output_1_acc: 0.9101 - val_auxilliary_output_2_acc: 0.9119\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2246 - output_loss: 0.1425 - auxilliary_output_1_loss: 0.1407 - auxilliary_output_2_loss: 0.1329 - output_acc: 0.9502 - auxilliary_output_1_acc: 0.9496 - auxilliary_output_2_acc: 0.9521 - val_loss: 0.5048 - val_output_loss: 0.3157 - val_auxilliary_output_1_loss: 0.3192 - val_auxilliary_output_2_loss: 0.3112 - val_output_acc: 0.9067 - val_auxilliary_output_1_acc: 0.9079 - val_auxilliary_output_2_acc: 0.9062\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2724 - output_loss: 0.1726 - auxilliary_output_1_loss: 0.1713 - auxilliary_output_2_loss: 0.1616 - output_acc: 0.9450 - auxilliary_output_1_acc: 0.9459 - auxilliary_output_2_acc: 0.9480 - val_loss: 0.5018 - val_output_loss: 0.3150 - val_auxilliary_output_1_loss: 0.3166 - val_auxilliary_output_2_loss: 0.3061 - val_output_acc: 0.9079 - val_auxilliary_output_1_acc: 0.9069 - val_auxilliary_output_2_acc: 0.9093\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1972 - output_loss: 0.1245 - auxilliary_output_1_loss: 0.1248 - auxilliary_output_2_loss: 0.1176 - output_acc: 0.9554 - auxilliary_output_1_acc: 0.9552 - auxilliary_output_2_acc: 0.9571 - val_loss: 0.4825 - val_output_loss: 0.3003 - val_auxilliary_output_1_loss: 0.3127 - val_auxilliary_output_2_loss: 0.2945 - val_output_acc: 0.9104 - val_auxilliary_output_1_acc: 0.9097 - val_auxilliary_output_2_acc: 0.9100\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2022 - output_loss: 0.1279 - auxilliary_output_1_loss: 0.1274 - auxilliary_output_2_loss: 0.1200 - output_acc: 0.9550 - auxilliary_output_1_acc: 0.9549 - auxilliary_output_2_acc: 0.9568 - val_loss: 0.5558 - val_output_loss: 0.3513 - val_auxilliary_output_1_loss: 0.3555 - val_auxilliary_output_2_loss: 0.3263 - val_output_acc: 0.9048 - val_auxilliary_output_1_acc: 0.9040 - val_auxilliary_output_2_acc: 0.9077\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2003 - output_loss: 0.1270 - auxilliary_output_1_loss: 0.1250 - auxilliary_output_2_loss: 0.1192 - output_acc: 0.9542 - auxilliary_output_1_acc: 0.9555 - auxilliary_output_2_acc: 0.9560 - val_loss: 0.5277 - val_output_loss: 0.3323 - val_auxilliary_output_1_loss: 0.3353 - val_auxilliary_output_2_loss: 0.3160 - val_output_acc: 0.9056 - val_auxilliary_output_1_acc: 0.9067 - val_auxilliary_output_2_acc: 0.9082\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2223 - output_loss: 0.1405 - auxilliary_output_1_loss: 0.1415 - auxilliary_output_2_loss: 0.1313 - output_acc: 0.9513 - auxilliary_output_1_acc: 0.9513 - auxilliary_output_2_acc: 0.9528 - val_loss: 0.4927 - val_output_loss: 0.3090 - val_auxilliary_output_1_loss: 0.3134 - val_auxilliary_output_2_loss: 0.2988 - val_output_acc: 0.9067 - val_auxilliary_output_1_acc: 0.9093 - val_auxilliary_output_2_acc: 0.9093\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.2024 - output_loss: 0.1292 - auxilliary_output_1_loss: 0.1257 - auxilliary_output_2_loss: 0.1184 - output_acc: 0.9561 - auxilliary_output_1_acc: 0.9554 - auxilliary_output_2_acc: 0.9567 - val_loss: 0.5180 - val_output_loss: 0.3257 - val_auxilliary_output_1_loss: 0.3318 - val_auxilliary_output_2_loss: 0.3091 - val_output_acc: 0.9121 - val_auxilliary_output_1_acc: 0.9110 - val_auxilliary_output_2_acc: 0.9136\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1732 - output_loss: 0.1097 - auxilliary_output_1_loss: 0.1092 - auxilliary_output_2_loss: 0.1026 - output_acc: 0.9620 - auxilliary_output_1_acc: 0.9607 - auxilliary_output_2_acc: 0.9631 - val_loss: 0.5369 - val_output_loss: 0.3287 - val_auxilliary_output_1_loss: 0.3542 - val_auxilliary_output_2_loss: 0.3399 - val_output_acc: 0.8989 - val_auxilliary_output_1_acc: 0.8981 - val_auxilliary_output_2_acc: 0.9000\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1715 - output_loss: 0.1087 - auxilliary_output_1_loss: 0.1077 - auxilliary_output_2_loss: 0.1015 - output_acc: 0.9621 - auxilliary_output_1_acc: 0.9617 - auxilliary_output_2_acc: 0.9636 - val_loss: 0.5436 - val_output_loss: 0.3404 - val_auxilliary_output_1_loss: 0.3498 - val_auxilliary_output_2_loss: 0.3276 - val_output_acc: 0.9120 - val_auxilliary_output_1_acc: 0.9103 - val_auxilliary_output_2_acc: 0.9125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1639 - output_loss: 0.1041 - auxilliary_output_1_loss: 0.1027 - auxilliary_output_2_loss: 0.0967 - output_acc: 0.9636 - auxilliary_output_1_acc: 0.9634 - auxilliary_output_2_acc: 0.9644 - val_loss: 0.5506 - val_output_loss: 0.3451 - val_auxilliary_output_1_loss: 0.3480 - val_auxilliary_output_2_loss: 0.3373 - val_output_acc: 0.9084 - val_auxilliary_output_1_acc: 0.9065 - val_auxilliary_output_2_acc: 0.9096\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1606 - output_loss: 0.1016 - auxilliary_output_1_loss: 0.1001 - auxilliary_output_2_loss: 0.0969 - output_acc: 0.9636 - auxilliary_output_1_acc: 0.9640 - auxilliary_output_2_acc: 0.9654 - val_loss: 0.5835 - val_output_loss: 0.3631 - val_auxilliary_output_1_loss: 0.3741 - val_auxilliary_output_2_loss: 0.3608 - val_output_acc: 0.9106 - val_auxilliary_output_1_acc: 0.9117 - val_auxilliary_output_2_acc: 0.9116\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1585 - output_loss: 0.1005 - auxilliary_output_1_loss: 0.1016 - auxilliary_output_2_loss: 0.0916 - output_acc: 0.9660 - auxilliary_output_1_acc: 0.9651 - auxilliary_output_2_acc: 0.9671 - val_loss: 0.6277 - val_output_loss: 0.3903 - val_auxilliary_output_1_loss: 0.4166 - val_auxilliary_output_2_loss: 0.3746 - val_output_acc: 0.9037 - val_auxilliary_output_1_acc: 0.9010 - val_auxilliary_output_2_acc: 0.9051\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1587 - output_loss: 0.0994 - auxilliary_output_1_loss: 0.1040 - auxilliary_output_2_loss: 0.0938 - output_acc: 0.9649 - auxilliary_output_1_acc: 0.9630 - auxilliary_output_2_acc: 0.9654 - val_loss: 0.5712 - val_output_loss: 0.3602 - val_auxilliary_output_1_loss: 0.3602 - val_auxilliary_output_2_loss: 0.3433 - val_output_acc: 0.9082 - val_auxilliary_output_1_acc: 0.9083 - val_auxilliary_output_2_acc: 0.9077\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1364 - output_loss: 0.0859 - auxilliary_output_1_loss: 0.0886 - auxilliary_output_2_loss: 0.0796 - output_acc: 0.9694 - auxilliary_output_1_acc: 0.9685 - auxilliary_output_2_acc: 0.9710 - val_loss: 0.6998 - val_output_loss: 0.4385 - val_auxilliary_output_1_loss: 0.4604 - val_auxilliary_output_2_loss: 0.4108 - val_output_acc: 0.9071 - val_auxilliary_output_1_acc: 0.9069 - val_auxilliary_output_2_acc: 0.9081\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1649 - output_loss: 0.1047 - auxilliary_output_1_loss: 0.1056 - auxilliary_output_2_loss: 0.0949 - output_acc: 0.9648 - auxilliary_output_1_acc: 0.9638 - auxilliary_output_2_acc: 0.9661 - val_loss: 0.5990 - val_output_loss: 0.3818 - val_auxilliary_output_1_loss: 0.3821 - val_auxilliary_output_2_loss: 0.3419 - val_output_acc: 0.9113 - val_auxilliary_output_1_acc: 0.9107 - val_auxilliary_output_2_acc: 0.9122\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1294 - output_loss: 0.0807 - auxilliary_output_1_loss: 0.0877 - auxilliary_output_2_loss: 0.0749 - output_acc: 0.9710 - auxilliary_output_1_acc: 0.9695 - auxilliary_output_2_acc: 0.9721 - val_loss: 0.6465 - val_output_loss: 0.4029 - val_auxilliary_output_1_loss: 0.4435 - val_auxilliary_output_2_loss: 0.3685 - val_output_acc: 0.9167 - val_auxilliary_output_1_acc: 0.9132 - val_auxilliary_output_2_acc: 0.9151\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1425 - output_loss: 0.0893 - auxilliary_output_1_loss: 0.0928 - auxilliary_output_2_loss: 0.0846 - output_acc: 0.9694 - auxilliary_output_1_acc: 0.9680 - auxilliary_output_2_acc: 0.9709 - val_loss: 0.5690 - val_output_loss: 0.3654 - val_auxilliary_output_1_loss: 0.3487 - val_auxilliary_output_2_loss: 0.3299 - val_output_acc: 0.9033 - val_auxilliary_output_1_acc: 0.9061 - val_auxilliary_output_2_acc: 0.9053\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1564 - output_loss: 0.1006 - auxilliary_output_1_loss: 0.0975 - auxilliary_output_2_loss: 0.0885 - output_acc: 0.9669 - auxilliary_output_1_acc: 0.9665 - auxilliary_output_2_acc: 0.9688 - val_loss: 0.5949 - val_output_loss: 0.3663 - val_auxilliary_output_1_loss: 0.3977 - val_auxilliary_output_2_loss: 0.3644 - val_output_acc: 0.9107 - val_auxilliary_output_1_acc: 0.9100 - val_auxilliary_output_2_acc: 0.9094\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1083 - output_loss: 0.0684 - auxilliary_output_1_loss: 0.0698 - auxilliary_output_2_loss: 0.0633 - output_acc: 0.9760 - auxilliary_output_1_acc: 0.9758 - auxilliary_output_2_acc: 0.9774 - val_loss: 0.6883 - val_output_loss: 0.4268 - val_auxilliary_output_1_loss: 0.4679 - val_auxilliary_output_2_loss: 0.4037 - val_output_acc: 0.9013 - val_auxilliary_output_1_acc: 0.9018 - val_auxilliary_output_2_acc: 0.9030\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1450 - output_loss: 0.0908 - auxilliary_output_1_loss: 0.0964 - auxilliary_output_2_loss: 0.0845 - output_acc: 0.9686 - auxilliary_output_1_acc: 0.9674 - auxilliary_output_2_acc: 0.9702 - val_loss: 0.6045 - val_output_loss: 0.3726 - val_auxilliary_output_1_loss: 0.3962 - val_auxilliary_output_2_loss: 0.3769 - val_output_acc: 0.9102 - val_auxilliary_output_1_acc: 0.9089 - val_auxilliary_output_2_acc: 0.9108\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1686 - output_loss: 0.1064 - auxilliary_output_1_loss: 0.1105 - auxilliary_output_2_loss: 0.0969 - output_acc: 0.9650 - auxilliary_output_1_acc: 0.9631 - auxilliary_output_2_acc: 0.9665 - val_loss: 0.5759 - val_output_loss: 0.3558 - val_auxilliary_output_1_loss: 0.3973 - val_auxilliary_output_2_loss: 0.3365 - val_output_acc: 0.9022 - val_auxilliary_output_1_acc: 0.9031 - val_auxilliary_output_2_acc: 0.9038\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1270 - output_loss: 0.0806 - auxilliary_output_1_loss: 0.0819 - auxilliary_output_2_loss: 0.0730 - output_acc: 0.9720 - auxilliary_output_1_acc: 0.9720 - auxilliary_output_2_acc: 0.9740 - val_loss: 0.6319 - val_output_loss: 0.3957 - val_auxilliary_output_1_loss: 0.4103 - val_auxilliary_output_2_loss: 0.3772 - val_output_acc: 0.9072 - val_auxilliary_output_1_acc: 0.9070 - val_auxilliary_output_2_acc: 0.9048\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1264 - output_loss: 0.0757 - auxilliary_output_1_loss: 0.0749 - auxilliary_output_2_loss: 0.0939 - output_acc: 0.9753 - auxilliary_output_1_acc: 0.9750 - auxilliary_output_2_acc: 0.9745 - val_loss: 0.6778 - val_output_loss: 0.4134 - val_auxilliary_output_1_loss: 0.4643 - val_auxilliary_output_2_loss: 0.4171 - val_output_acc: 0.9084 - val_auxilliary_output_1_acc: 0.9076 - val_auxilliary_output_2_acc: 0.9086\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1218 - output_loss: 0.0764 - auxilliary_output_1_loss: 0.0822 - auxilliary_output_2_loss: 0.0693 - output_acc: 0.9747 - auxilliary_output_1_acc: 0.9735 - auxilliary_output_2_acc: 0.9759 - val_loss: 0.7138 - val_output_loss: 0.4412 - val_auxilliary_output_1_loss: 0.5157 - val_auxilliary_output_2_loss: 0.3930 - val_output_acc: 0.9048 - val_auxilliary_output_1_acc: 0.9035 - val_auxilliary_output_2_acc: 0.9053\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.1093 - output_loss: 0.0682 - auxilliary_output_1_loss: 0.0748 - auxilliary_output_2_loss: 0.0624 - output_acc: 0.9768 - auxilliary_output_1_acc: 0.9754 - auxilliary_output_2_acc: 0.9779 - val_loss: 0.7529 - val_output_loss: 0.4676 - val_auxilliary_output_1_loss: 0.5255 - val_auxilliary_output_2_loss: 0.4256 - val_output_acc: 0.9081 - val_auxilliary_output_1_acc: 0.9083 - val_auxilliary_output_2_acc: 0.9102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207da080a90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,[train_y,train_y,train_y], validation_data = (val_x, [val_y,val_y,val_y]), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9097"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred[0], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9099"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred[1], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9108"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred[2], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding=\"same\",input_shape=(28,28,1)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPool2D((2,2), strides=(2,2), padding=\"same\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding=\"same\"))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding=\"same\"))\n",
    "model.add(MaxPool2D((2,2), strides=(2,2), padding=\"same\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Conv2D(256, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(Conv2D(256, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(Conv2D(256, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(MaxPool2D((2,2), strides=(2,2), padding=\"same\"))\n",
    "\n",
    "# model.add(Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(MaxPool2D((2,2), strides=(2,2), padding=\"same\"))\n",
    "\n",
    "# model.add(Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(Conv2D(512, (3,3), activation='relu', padding=\"same\"))\n",
    "# model.add(MaxPool2D((2,2), strides=(2,2), padding=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_179 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 3,667,722\n",
      "Trainable params: 3,667,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 27s 556us/step - loss: 0.6217 - acc: 0.7725 - val_loss: 0.3248 - val_acc: 0.8818\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 24s 509us/step - loss: 0.3955 - acc: 0.8605 - val_loss: 0.2826 - val_acc: 0.9004\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 25s 515us/step - loss: 0.3479 - acc: 0.8767 - val_loss: 0.2503 - val_acc: 0.9111\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.3187 - acc: 0.8881 - val_loss: 0.2466 - val_acc: 0.9087\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 25s 517us/step - loss: 0.3028 - acc: 0.8927 - val_loss: 0.2269 - val_acc: 0.9194\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 25s 516us/step - loss: 0.2946 - acc: 0.8961 - val_loss: 0.2211 - val_acc: 0.9172\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.2821 - acc: 0.9006 - val_loss: 0.2218 - val_acc: 0.9190\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2725 - acc: 0.9038 - val_loss: 0.2322 - val_acc: 0.9161\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 25s 515us/step - loss: 0.2694 - acc: 0.9042 - val_loss: 0.2178 - val_acc: 0.9206\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 25s 511us/step - loss: 0.2672 - acc: 0.9060 - val_loss: 0.2185 - val_acc: 0.9228\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 25s 517us/step - loss: 0.2606 - acc: 0.9083 - val_loss: 0.2129 - val_acc: 0.9227\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2634 - acc: 0.9068 - val_loss: 0.2131 - val_acc: 0.9238\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 25s 517us/step - loss: 0.2546 - acc: 0.9110 - val_loss: 0.2086 - val_acc: 0.9243\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 25s 516us/step - loss: 0.2510 - acc: 0.9112 - val_loss: 0.2086 - val_acc: 0.9259\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 24s 507us/step - loss: 0.2482 - acc: 0.9125 - val_loss: 0.2081 - val_acc: 0.9253\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 24s 509us/step - loss: 0.2480 - acc: 0.9116 - val_loss: 0.2153 - val_acc: 0.9221\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.2501 - acc: 0.9113 - val_loss: 0.2259 - val_acc: 0.9173\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 25s 518us/step - loss: 0.2418 - acc: 0.9146 - val_loss: 0.2060 - val_acc: 0.9268\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 25s 513us/step - loss: 0.2401 - acc: 0.9145 - val_loss: 0.2073 - val_acc: 0.9281\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 25s 511us/step - loss: 0.2392 - acc: 0.9153 - val_loss: 0.1974 - val_acc: 0.9291\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 25s 520us/step - loss: 0.2410 - acc: 0.9153 - val_loss: 0.2134 - val_acc: 0.9226\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 25s 513us/step - loss: 0.2395 - acc: 0.9160 - val_loss: 0.2015 - val_acc: 0.9263\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.2341 - acc: 0.9183 - val_loss: 0.2024 - val_acc: 0.9288\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2318 - acc: 0.9185 - val_loss: 0.2003 - val_acc: 0.9316\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 24s 507us/step - loss: 0.2360 - acc: 0.9179 - val_loss: 0.2165 - val_acc: 0.9221\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 25s 515us/step - loss: 0.2343 - acc: 0.9188 - val_loss: 0.2125 - val_acc: 0.9213\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.2300 - acc: 0.9191 - val_loss: 0.2047 - val_acc: 0.9293\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 25s 515us/step - loss: 0.2288 - acc: 0.9206 - val_loss: 0.2014 - val_acc: 0.9308\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 25s 513us/step - loss: 0.2259 - acc: 0.9220 - val_loss: 0.2071 - val_acc: 0.9263\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 25s 514us/step - loss: 0.2266 - acc: 0.9210 - val_loss: 0.2062 - val_acc: 0.9313\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 24s 508us/step - loss: 0.2263 - acc: 0.9218 - val_loss: 0.2000 - val_acc: 0.9306\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 24s 506us/step - loss: 0.2283 - acc: 0.9194 - val_loss: 0.2023 - val_acc: 0.9278\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 24s 509us/step - loss: 0.2306 - acc: 0.9186 - val_loss: 0.2042 - val_acc: 0.9294\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 25s 511us/step - loss: 0.2291 - acc: 0.9218 - val_loss: 0.2044 - val_acc: 0.9268\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2228 - acc: 0.9230 - val_loss: 0.2009 - val_acc: 0.9313\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 25s 513us/step - loss: 0.2228 - acc: 0.9223 - val_loss: 0.2031 - val_acc: 0.9318\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 25s 515us/step - loss: 0.2237 - acc: 0.9246 - val_loss: 0.2090 - val_acc: 0.9253\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2228 - acc: 0.9231 - val_loss: 0.2070 - val_acc: 0.9294\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 24s 510us/step - loss: 0.2215 - acc: 0.9235 - val_loss: 0.2066 - val_acc: 0.9283\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2254 - acc: 0.9211 - val_loss: 0.2120 - val_acc: 0.9272\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 25s 511us/step - loss: 0.2229 - acc: 0.9231 - val_loss: 0.2187 - val_acc: 0.9230\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 24s 508us/step - loss: 0.2210 - acc: 0.9226 - val_loss: 0.1940 - val_acc: 0.9317\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2261 - acc: 0.9249 - val_loss: 0.2029 - val_acc: 0.9313\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 25s 513us/step - loss: 0.2238 - acc: 0.9231 - val_loss: 0.2046 - val_acc: 0.9298\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 25s 516us/step - loss: 0.2223 - acc: 0.9228 - val_loss: 0.2044 - val_acc: 0.9306\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2182 - acc: 0.9226 - val_loss: 0.2129 - val_acc: 0.9288\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 25s 515us/step - loss: 0.2220 - acc: 0.9231 - val_loss: 0.1965 - val_acc: 0.9347\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 24s 508us/step - loss: 0.2212 - acc: 0.9245 - val_loss: 0.2083 - val_acc: 0.9310\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 25s 512us/step - loss: 0.2196 - acc: 0.9244 - val_loss: 0.2047 - val_acc: 0.9317\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 25s 519us/step - loss: 0.2214 - acc: 0.9239 - val_loss: 0.2155 - val_acc: 0.9274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207f4802a90>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    def residual_module(data, K, stride, chanDim, red=False, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "        shortcut = data\n",
    "        \n",
    "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(data)\n",
    "        act1 = Activation(\"relu\")(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "        \n",
    "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv1)\n",
    "        act2 = Activation(\"relu\")(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride, padding=\"same\", use_bias=False, kernel_regularizer=l2(reg))(act2)\n",
    "                       \n",
    "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(conv2)\n",
    "        act3 = Activation(\"relu\")(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act3)\n",
    "                       \n",
    "        if red:\n",
    "            shortcut = Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "        x = add([conv3, shortcut])\n",
    "\n",
    "        return x\n",
    "                  \n",
    "    def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9):             \n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "                       \n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(inputs)\n",
    "\n",
    "        # apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "        x = Conv2D(filters[0], (5, 5), use_bias=False,padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = ZeroPadding2D((1, 1))(x)\n",
    "        x = MaxPool2D((3, 3), strides=(2, 2))(x)\n",
    "                       \n",
    "        \n",
    "        # loop over the number of stages\n",
    "        for i in range(0, len(stages)):\n",
    "            stride = (1, 1) if i == 0 else (2, 2)\n",
    "            x = ResNet.residual_module(x, filters[i + 1], stride,chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "            # loop over the number of layers in the stage\n",
    "            for j in range(0, stages[i] - 1):\n",
    "                # apply a ResNet module\n",
    "                x = ResNet.residual_module(x, filters[i + 1],(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "            \n",
    "        # apply BN => ACT => POOL\n",
    "        x = BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        #x = AveragePooling2D((8, 8))(x)\n",
    "        x = AveragePooling2D((4, 4))(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "        x = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ResNet.build(28,28,1,10,stages=(3, 4, 6),filters=(64, 128, 256, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 28, 28, 1)    4           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 28, 28, 64)   1600        batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 28, 28, 64)   256         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 28, 28, 64)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 30, 30, 64)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling2D) (None, 14, 14, 64)   0           zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 64)   256         max_pooling2d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 14, 14, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 14, 14, 32)   2048        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 14, 14, 32)   128         conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 14, 14, 32)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 14, 14, 32)   9216        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 14, 14, 32)   128         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 14, 14, 32)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 14, 14, 128)  4096        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 14, 14, 128)  8192        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 14, 14, 128)  0           conv2d_193[0][0]                 \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 14, 14, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 14, 14, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 14, 14, 32)   4096        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 14, 14, 32)   128         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 14, 14, 32)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 14, 14, 32)   9216        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 14, 14, 32)   128         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 14, 14, 32)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 14, 14, 128)  4096        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 14, 14, 128)  0           conv2d_197[0][0]                 \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 14, 14, 128)  512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 14, 14, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 14, 14, 32)   4096        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 14, 14, 32)   128         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 14, 14, 32)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 14, 14, 32)   9216        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 14, 14, 32)   128         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 14, 14, 32)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 14, 14, 128)  4096        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 14, 14, 128)  0           conv2d_200[0][0]                 \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 14, 14, 128)  512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 14, 14, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 14, 14, 64)   8192        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 14, 14, 64)   256         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 14, 14, 64)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 7, 7, 64)     36864       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 64)     256         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 7, 7, 64)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 7, 7, 256)    16384       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 7, 7, 256)    32768       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 7, 7, 256)    0           conv2d_203[0][0]                 \n",
      "                                                                 conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 256)    1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 7, 7, 256)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 7, 7, 64)     16384       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 7, 7, 64)     256         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 7, 7, 64)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 7, 7, 64)     36864       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 7, 7, 64)     256         conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 7, 7, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 7, 7, 256)    16384       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 7, 7, 256)    0           conv2d_207[0][0]                 \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 7, 7, 256)    1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 7, 7, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 7, 7, 64)     16384       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 7, 7, 64)     256         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 7, 7, 64)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 7, 7, 64)     36864       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 7, 7, 64)     256         conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 7, 7, 64)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 7, 7, 256)    16384       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 7, 7, 256)    0           conv2d_210[0][0]                 \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 7, 7, 256)    1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 7, 7, 256)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 7, 7, 64)     16384       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 7, 7, 64)     256         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 7, 7, 64)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 7, 7, 64)     36864       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 7, 7, 64)     256         conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 7, 7, 64)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 7, 7, 256)    16384       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 7, 7, 256)    0           conv2d_213[0][0]                 \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 7, 7, 256)    1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 7, 7, 256)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 7, 7, 128)    32768       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 7, 7, 128)    512         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 7, 7, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 4, 4, 128)    147456      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 128)    512         conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 128)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 4, 4, 512)    65536       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 4, 4, 512)    131072      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 4, 4, 512)    0           conv2d_216[0][0]                 \n",
      "                                                                 conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 512)    2048        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 512)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 4, 4, 128)    65536       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 128)    512         conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 128)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 4, 4, 128)    147456      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 128)    512         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 128)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 4, 4, 512)    65536       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 512)    0           conv2d_220[0][0]                 \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 512)    2048        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 512)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 4, 4, 128)    65536       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 4, 4, 128)    512         conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 128)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 4, 4, 128)    147456      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 4, 4, 128)    512         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 128)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 4, 4, 512)    65536       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 512)    0           conv2d_223[0][0]                 \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 4, 4, 512)    2048        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 512)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 4, 4, 128)    65536       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 4, 4, 128)    512         conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 128)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 4, 4, 128)    147456      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 4, 4, 128)    512         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 128)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 4, 4, 512)    65536       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 512)    0           conv2d_226[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 4, 4, 512)    2048        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 512)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 4, 4, 128)    65536       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 4, 4, 128)    512         conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 128)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 4, 4, 128)    147456      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 4, 4, 128)    512         conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 128)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 4, 4, 512)    65536       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 512)    0           conv2d_229[0][0]                 \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 4, 4, 512)    2048        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 512)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 4, 4, 128)    65536       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 4, 4, 128)    512         conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 128)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 4, 4, 128)    147456      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 4, 4, 128)    512         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 128)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 4, 4, 512)    65536       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 4, 512)    0           conv2d_232[0][0]                 \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 4, 4, 512)    2048        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 512)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 10)           5130        flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 10)           0           dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,165,070\n",
      "Trainable params: 2,151,372\n",
      "Non-trainable params: 13,698\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 78s 2ms/step - loss: 0.8873 - acc: 0.8403 - val_loss: 0.6935 - val_acc: 0.8722\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.5912 - acc: 0.8872 - val_loss: 0.5235 - val_acc: 0.8926\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.4920 - acc: 0.8972 - val_loss: 0.4649 - val_acc: 0.8987\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.4390 - acc: 0.9041 - val_loss: 0.4185 - val_acc: 0.9066\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.4111 - acc: 0.9088 - val_loss: 0.4054 - val_acc: 0.9087\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.3883 - acc: 0.9125 - val_loss: 0.4303 - val_acc: 0.8956\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.3718 - acc: 0.9161 - val_loss: 0.3678 - val_acc: 0.9185\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.3591 - acc: 0.9204 - val_loss: 0.3648 - val_acc: 0.9196\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.3462 - acc: 0.9209 - val_loss: 0.3751 - val_acc: 0.9122\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.3354 - acc: 0.9268 - val_loss: 0.3581 - val_acc: 0.9165\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.3271 - acc: 0.9272 - val_loss: 0.3620 - val_acc: 0.9165\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.3182 - acc: 0.9305 - val_loss: 0.3678 - val_acc: 0.9159\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.3097 - acc: 0.9321 - val_loss: 0.3593 - val_acc: 0.9205\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.3056 - acc: 0.9346 - val_loss: 0.4133 - val_acc: 0.8957\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2984 - acc: 0.9363 - val_loss: 0.3345 - val_acc: 0.9272\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2929 - acc: 0.9378 - val_loss: 0.3660 - val_acc: 0.9162\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2877 - acc: 0.9408 - val_loss: 0.3410 - val_acc: 0.9247\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2799 - acc: 0.9431 - val_loss: 0.3633 - val_acc: 0.9183\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2775 - acc: 0.9436 - val_loss: 0.3457 - val_acc: 0.9224\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2696 - acc: 0.9470 - val_loss: 0.3616 - val_acc: 0.9157\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2667 - acc: 0.9473 - val_loss: 0.3432 - val_acc: 0.9242\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2665 - acc: 0.9481 - val_loss: 0.3580 - val_acc: 0.9188\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2583 - acc: 0.9502 - val_loss: 0.3655 - val_acc: 0.9158\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2552 - acc: 0.9537 - val_loss: 0.3603 - val_acc: 0.9221\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2564 - acc: 0.9520 - val_loss: 0.3436 - val_acc: 0.9242\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2520 - acc: 0.9533 - val_loss: 0.3831 - val_acc: 0.9140\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2480 - acc: 0.9553 - val_loss: 0.3750 - val_acc: 0.9180\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2449 - acc: 0.9564 - val_loss: 0.3472 - val_acc: 0.9287\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2428 - acc: 0.9572 - val_loss: 0.3662 - val_acc: 0.9211\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2386 - acc: 0.9581 - val_loss: 0.3818 - val_acc: 0.9177\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2374 - acc: 0.9591 - val_loss: 0.3719 - val_acc: 0.9216\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2390 - acc: 0.9585 - val_loss: 0.3611 - val_acc: 0.9255\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2356 - acc: 0.9604 - val_loss: 0.4398 - val_acc: 0.9059\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2286 - acc: 0.9626 - val_loss: 0.3755 - val_acc: 0.9203\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2294 - acc: 0.9627 - val_loss: 0.4251 - val_acc: 0.9087\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2292 - acc: 0.9632 - val_loss: 0.3779 - val_acc: 0.9193\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2251 - acc: 0.9649 - val_loss: 0.3733 - val_acc: 0.9226\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2256 - acc: 0.9638 - val_loss: 0.3787 - val_acc: 0.9227\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 70s 1ms/step - loss: 0.2281 - acc: 0.9635 - val_loss: 0.3900 - val_acc: 0.9159\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2224 - acc: 0.9656 - val_loss: 0.3796 - val_acc: 0.9207\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 72s 1ms/step - loss: 0.2242 - acc: 0.9637 - val_loss: 0.3804 - val_acc: 0.9187\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 72s 2ms/step - loss: 0.2184 - acc: 0.9671 - val_loss: 0.3776 - val_acc: 0.9236\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2204 - acc: 0.9659 - val_loss: 0.3915 - val_acc: 0.9174\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 72s 1ms/step - loss: 0.2192 - acc: 0.9662 - val_loss: 0.3909 - val_acc: 0.9197\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2190 - acc: 0.9663 - val_loss: 0.3826 - val_acc: 0.9226\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2158 - acc: 0.9674 - val_loss: 0.3867 - val_acc: 0.9212\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2130 - acc: 0.9679 - val_loss: 0.4072 - val_acc: 0.9166\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2148 - acc: 0.9676 - val_loss: 0.3968 - val_acc: 0.9203\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2133 - acc: 0.9679 - val_loss: 0.3904 - val_acc: 0.9230\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 71s 1ms/step - loss: 0.2112 - acc: 0.9686 - val_loss: 0.3870 - val_acc: 0.9218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x207f9e10a58>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseNet(img_size=(28, 28, 1), classes=10, nb_filter=64, nb_layers = [6,12,24,16], growth_rate=32, concat_axis = -1, compression=1.0, dropout_rate=0.0, eps = 1.1e-5):\n",
    "\n",
    "    img_input = Input(shape=img_size, name='data')\n",
    "\n",
    "    # Initial convolution\n",
    "    x = Conv2D(filters=nb_filter, kernel_size=7, strides=2, padding='same')(img_input)\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool2D((3, 3), padding='same', strides=(2, 2))(x)\n",
    "\n",
    "    #For len(nb_layers)-1 create dense_block=>transition_block \n",
    "    for block_idx in range(len(nb_layers) - 1):\n",
    "        x, nb_filter = dense_block(x, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "        x = transition_block(x, nb_filter, compression=compression, dropout_rate=dropout_rate)\n",
    "        nb_filter = int(nb_filter * compression)\n",
    "\n",
    "    #Final dense block has no transition block\n",
    "    x, nb_filter = dense_block(x, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dense(classes)(x)\n",
    "    x = Activation('softmax', name='prob')(x)\n",
    "\n",
    "    model = Model(img_input, x, name='densenet')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def conv_block(x, nb_filter, dropout_rate=None, eps = 1.1e-5, concat_axis = -1):\n",
    "\n",
    "    #bn=>relu=>1x1conv=>dropout\n",
    "    inter_channel = nb_filter * 4  \n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters=inter_channel, kernel_size=1, strides=1, padding='same')(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "    #bn=>relu=>conv2d(same)=>dropout\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters=nb_filter, kernel_size=3, strides=1, padding='same')(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def transition_block(x, nb_filter, compression=1.0, dropout_rate=None, eps = 1.1e-5, concat_axis = -1):\n",
    "    \n",
    "    #bn=>relu=>conv2d(controls #of channels(compression=percentage))=>avgpool(divide by 2)\n",
    "    x = BatchNormalization(epsilon=eps, axis=concat_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters=int(nb_filter * compression), kernel_size=1, strides=1, padding='same')(x)\n",
    "\n",
    "    if dropout_rate:\n",
    "        x = Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "    x = AveragePooling2D((2, 2), strides=2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_block(x, nb_layers, nb_filter, growth_rate, dropout_rate=None, eps = 1.1e-5, concat_axis = -1):\n",
    "\n",
    "    conv_list=[x]\n",
    "    for i in range(nb_layers):\n",
    "        x = conv_block(x, growth_rate, dropout_rate)\n",
    "        conv_list.append(x)\n",
    "        x = concatenate(conv_list, axis=concat_axis)\n",
    "        nb_filter += growth_rate\n",
    "\n",
    "    return x, nb_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DenseNet(nb_layers = [6,12,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 14, 14, 64)   3200        data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 14, 14, 64)   256         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 14, 14, 64)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_52 (MaxPooling2D) (None, 7, 7, 64)     0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 7, 7, 64)     256         max_pooling2d_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 7, 7, 64)     0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 7, 7, 128)    8320        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 7, 7, 128)    512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 7, 7, 128)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 7, 7, 32)     36896       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7, 7, 96)     0           max_pooling2d_52[0][0]           \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 7, 7, 96)     384         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 7, 7, 96)     0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 7, 7, 128)    12416       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 7, 7, 128)    512         conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 7, 7, 128)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 7, 7, 32)     36896       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 7, 7, 128)    0           max_pooling2d_52[0][0]           \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "                                                                 conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 7, 7, 128)    512         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 7, 7, 128)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 7, 7, 128)    16512       activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 7, 7, 128)    512         conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 7, 7, 128)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 7, 7, 32)     36896       activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7, 7, 160)    0           max_pooling2d_52[0][0]           \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "                                                                 conv2d_238[0][0]                 \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 7, 7, 160)    640         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 7, 7, 128)    20608       activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 7, 7, 128)    512         conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 7, 7, 128)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 7, 7, 32)     36896       activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7, 7, 192)    0           max_pooling2d_52[0][0]           \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "                                                                 conv2d_238[0][0]                 \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "                                                                 conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 7, 7, 192)    768         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 7, 7, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 7, 7, 128)    24704       activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 7, 7, 128)    512         conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 7, 7, 128)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 7, 7, 32)     36896       activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 7, 7, 224)    0           max_pooling2d_52[0][0]           \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "                                                                 conv2d_238[0][0]                 \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "                                                                 conv2d_242[0][0]                 \n",
      "                                                                 conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 7, 7, 224)    896         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 7, 7, 224)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 7, 7, 128)    28800       activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 7, 7, 128)    512         conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 7, 7, 128)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 7, 7, 32)     36896       activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 7, 7, 256)    0           max_pooling2d_52[0][0]           \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "                                                                 conv2d_238[0][0]                 \n",
      "                                                                 conv2d_240[0][0]                 \n",
      "                                                                 conv2d_242[0][0]                 \n",
      "                                                                 conv2d_244[0][0]                 \n",
      "                                                                 conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 7, 7, 256)    1024        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 7, 7, 256)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 7, 7, 256)    65792       activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 3, 3, 256)    0           conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 3, 3, 256)    1024        average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 3, 3, 256)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 3, 3, 128)    32896       activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 3, 3, 128)    512         conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 3, 3, 128)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)             (None, 3, 3, 32)     36896       activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 3, 3, 288)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 3, 3, 288)    1152        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 3, 3, 288)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)             (None, 3, 3, 128)    36992       activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 3, 3, 128)    512         conv2d_250[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 3, 3, 128)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)             (None, 3, 3, 32)     36896       activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 3, 3, 320)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 3, 3, 320)    1280        concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 3, 3, 320)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)             (None, 3, 3, 128)    41088       activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 3, 3, 128)    512         conv2d_252[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 3, 3, 128)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)             (None, 3, 3, 32)     36896       activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 3, 3, 352)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 3, 3, 352)    1408        concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 3, 3, 352)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)             (None, 3, 3, 128)    45184       activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 3, 3, 128)    512         conv2d_254[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 3, 3, 128)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)             (None, 3, 3, 32)     36896       activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 3, 3, 384)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1536        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)             (None, 3, 3, 128)    49280       activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 3, 3, 128)    512         conv2d_256[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 3, 3, 128)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)             (None, 3, 3, 32)     36896       activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 3, 3, 416)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 3, 3, 416)    1664        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 3, 3, 416)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)             (None, 3, 3, 128)    53376       activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 3, 3, 128)    512         conv2d_258[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 3, 3, 128)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)             (None, 3, 3, 32)     36896       activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 3, 3, 448)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 3, 3, 448)    1792        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 3, 3, 448)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)             (None, 3, 3, 128)    57472       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 3, 3, 128)    512         conv2d_260[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 3, 3, 128)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)             (None, 3, 3, 32)     36896       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 3, 3, 480)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "                                                                 conv2d_261[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 3, 3, 480)    1920        concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 3, 3, 480)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)             (None, 3, 3, 128)    61568       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 3, 3, 128)    512         conv2d_262[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 3, 3, 128)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)             (None, 3, 3, 32)     36896       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 3, 3, 512)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "                                                                 conv2d_261[0][0]                 \n",
      "                                                                 conv2d_263[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 3, 3, 512)    2048        concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 3, 3, 512)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)             (None, 3, 3, 128)    65664       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 3, 3, 128)    512         conv2d_264[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 3, 3, 128)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)             (None, 3, 3, 32)     36896       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 3, 3, 544)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "                                                                 conv2d_261[0][0]                 \n",
      "                                                                 conv2d_263[0][0]                 \n",
      "                                                                 conv2d_265[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 3, 3, 544)    2176        concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 3, 3, 544)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)             (None, 3, 3, 128)    69760       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 3, 3, 128)    512         conv2d_266[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 3, 3, 128)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)             (None, 3, 3, 32)     36896       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 3, 3, 576)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "                                                                 conv2d_261[0][0]                 \n",
      "                                                                 conv2d_263[0][0]                 \n",
      "                                                                 conv2d_265[0][0]                 \n",
      "                                                                 conv2d_267[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 3, 3, 576)    2304        concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 3, 3, 576)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)             (None, 3, 3, 128)    73856       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 3, 3, 128)    512         conv2d_268[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 3, 3, 128)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)             (None, 3, 3, 32)     36896       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 3, 3, 608)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "                                                                 conv2d_261[0][0]                 \n",
      "                                                                 conv2d_263[0][0]                 \n",
      "                                                                 conv2d_265[0][0]                 \n",
      "                                                                 conv2d_267[0][0]                 \n",
      "                                                                 conv2d_269[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 3, 3, 608)    2432        concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 3, 3, 608)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)             (None, 3, 3, 128)    77952       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 3, 3, 128)    512         conv2d_270[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 3, 3, 128)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)             (None, 3, 3, 32)     36896       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 3, 3, 640)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_249[0][0]                 \n",
      "                                                                 conv2d_251[0][0]                 \n",
      "                                                                 conv2d_253[0][0]                 \n",
      "                                                                 conv2d_255[0][0]                 \n",
      "                                                                 conv2d_257[0][0]                 \n",
      "                                                                 conv2d_259[0][0]                 \n",
      "                                                                 conv2d_261[0][0]                 \n",
      "                                                                 conv2d_263[0][0]                 \n",
      "                                                                 conv2d_265[0][0]                 \n",
      "                                                                 conv2d_267[0][0]                 \n",
      "                                                                 conv2d_269[0][0]                 \n",
      "                                                                 conv2d_271[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 3, 3, 640)    2560        concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 3, 3, 640)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)             (None, 3, 3, 640)    410240      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 640)    0           conv2d_272[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 1, 1, 640)    2560        average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 1, 1, 640)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)             (None, 1, 1, 128)    82048       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 1, 1, 128)    512         conv2d_273[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 1, 1, 128)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)             (None, 1, 1, 32)     36896       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 1, 1, 672)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 1, 1, 672)    2688        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 1, 1, 672)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)             (None, 1, 1, 128)    86144       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 1, 1, 128)    512         conv2d_275[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 1, 1, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)             (None, 1, 1, 32)     36896       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 1, 1, 704)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 1, 1, 704)    2816        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 1, 1, 704)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)             (None, 1, 1, 128)    90240       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 1, 1, 128)    512         conv2d_277[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 1, 1, 128)    0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)             (None, 1, 1, 32)     36896       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 1, 1, 736)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 1, 1, 736)    2944        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 1, 1, 736)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)             (None, 1, 1, 128)    94336       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 1, 1, 128)    512         conv2d_279[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 1, 1, 128)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)             (None, 1, 1, 32)     36896       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 1, 1, 768)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 1, 1, 768)    3072        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 1, 1, 768)    0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)             (None, 1, 1, 128)    98432       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 1, 1, 128)    512         conv2d_281[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 1, 1, 128)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)             (None, 1, 1, 32)     36896       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 1, 1, 800)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 1, 1, 800)    3200        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 1, 1, 800)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_283 (Conv2D)             (None, 1, 1, 128)    102528      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 1, 1, 128)    512         conv2d_283[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 1, 1, 128)    0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_284 (Conv2D)             (None, 1, 1, 32)     36896       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1, 1, 832)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 1, 1, 832)    3328        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 1, 1, 832)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)             (None, 1, 1, 128)    106624      activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 1, 1, 128)    512         conv2d_285[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 1, 1, 128)    0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)             (None, 1, 1, 32)     36896       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 1, 1, 864)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 1, 1, 864)    3456        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 1, 1, 864)    0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)             (None, 1, 1, 128)    110720      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 1, 1, 128)    512         conv2d_287[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 1, 1, 128)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)             (None, 1, 1, 32)     36896       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 1, 1, 896)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 1, 1, 896)    3584        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 1, 1, 896)    0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_289 (Conv2D)             (None, 1, 1, 128)    114816      activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 1, 1, 128)    512         conv2d_289[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 1, 1, 128)    0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_290 (Conv2D)             (None, 1, 1, 32)     36896       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 1, 1, 928)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 1, 1, 928)    3712        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 1, 1, 928)    0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_291 (Conv2D)             (None, 1, 1, 128)    118912      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 1, 1, 128)    512         conv2d_291[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 1, 1, 128)    0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_292 (Conv2D)             (None, 1, 1, 32)     36896       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 1, 1, 960)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 1, 1, 960)    3840        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 1, 1, 960)    0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_293 (Conv2D)             (None, 1, 1, 128)    123008      activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 1, 1, 128)    512         conv2d_293[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 1, 1, 128)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_294 (Conv2D)             (None, 1, 1, 32)     36896       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 1, 1, 992)    0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 1, 1, 992)    3968        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 1, 1, 992)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_295 (Conv2D)             (None, 1, 1, 128)    127104      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 1, 1, 128)    512         conv2d_295[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 1, 1, 128)    0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_296 (Conv2D)             (None, 1, 1, 32)     36896       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 1, 1, 1024)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 1, 1, 1024)   4096        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 1, 1, 1024)   0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_297 (Conv2D)             (None, 1, 1, 128)    131200      activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 1, 1, 128)    512         conv2d_297[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 1, 1, 128)    0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_298 (Conv2D)             (None, 1, 1, 32)     36896       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 1, 1, 1056)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 1, 1, 1056)   4224        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 1, 1, 1056)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_299 (Conv2D)             (None, 1, 1, 128)    135296      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 1, 1, 128)    512         conv2d_299[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 1, 1, 128)    0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_300 (Conv2D)             (None, 1, 1, 32)     36896       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 1, 1, 1088)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 1, 1, 1088)   4352        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 1, 1, 1088)   0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_301 (Conv2D)             (None, 1, 1, 128)    139392      activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 1, 1, 128)    512         conv2d_301[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 1, 1, 128)    0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_302 (Conv2D)             (None, 1, 1, 32)     36896       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 1, 1, 1120)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 1, 1, 1120)   4480        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 1, 1, 1120)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_303 (Conv2D)             (None, 1, 1, 128)    143488      activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 1, 1, 128)    512         conv2d_303[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 1, 1, 128)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_304 (Conv2D)             (None, 1, 1, 32)     36896       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 1, 1, 1152)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 1, 1, 1152)   4608        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 1, 1, 1152)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_305 (Conv2D)             (None, 1, 1, 128)    147584      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 1, 1, 128)    512         conv2d_305[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 1, 1, 128)    0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_306 (Conv2D)             (None, 1, 1, 32)     36896       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 1, 1, 1184)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 1, 1, 1184)   4736        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 1, 1, 1184)   0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_307 (Conv2D)             (None, 1, 1, 128)    151680      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 1, 1, 128)    512         conv2d_307[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 1, 1, 128)    0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_308 (Conv2D)             (None, 1, 1, 32)     36896       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 1, 1, 1216)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 1, 1, 1216)   4864        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 1, 1, 1216)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_309 (Conv2D)             (None, 1, 1, 128)    155776      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 1, 1, 128)    512         conv2d_309[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 1, 1, 128)    0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_310 (Conv2D)             (None, 1, 1, 32)     36896       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 1, 1, 1248)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 1, 1, 1248)   4992        concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 1, 1, 1248)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_311 (Conv2D)             (None, 1, 1, 128)    159872      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 1, 1, 128)    512         conv2d_311[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 1, 1, 128)    0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_312 (Conv2D)             (None, 1, 1, 32)     36896       activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 1, 1, 1280)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "                                                                 conv2d_312[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 1, 1, 1280)   5120        concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 1, 1, 1280)   0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_313 (Conv2D)             (None, 1, 1, 128)    163968      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 1, 1, 128)    512         conv2d_313[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 1, 1, 128)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_314 (Conv2D)             (None, 1, 1, 32)     36896       activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 1, 1, 1312)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "                                                                 conv2d_312[0][0]                 \n",
      "                                                                 conv2d_314[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 1, 1, 1312)   5248        concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 1, 1, 1312)   0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_315 (Conv2D)             (None, 1, 1, 128)    168064      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 1, 1, 128)    512         conv2d_315[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 1, 1, 128)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_316 (Conv2D)             (None, 1, 1, 32)     36896       activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 1, 1, 1344)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "                                                                 conv2d_312[0][0]                 \n",
      "                                                                 conv2d_314[0][0]                 \n",
      "                                                                 conv2d_316[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 1, 1, 1344)   5376        concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 1, 1, 1344)   0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_317 (Conv2D)             (None, 1, 1, 128)    172160      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 1, 1, 128)    512         conv2d_317[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 1, 1, 128)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_318 (Conv2D)             (None, 1, 1, 32)     36896       activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 1, 1, 1376)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "                                                                 conv2d_312[0][0]                 \n",
      "                                                                 conv2d_314[0][0]                 \n",
      "                                                                 conv2d_316[0][0]                 \n",
      "                                                                 conv2d_318[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 1, 1, 1376)   5504        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 1, 1, 1376)   0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_319 (Conv2D)             (None, 1, 1, 128)    176256      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 1, 1, 128)    512         conv2d_319[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 1, 1, 128)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_320 (Conv2D)             (None, 1, 1, 32)     36896       activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 1, 1, 1408)   0           average_pooling2d_3[0][0]        \n",
      "                                                                 conv2d_274[0][0]                 \n",
      "                                                                 conv2d_276[0][0]                 \n",
      "                                                                 conv2d_278[0][0]                 \n",
      "                                                                 conv2d_280[0][0]                 \n",
      "                                                                 conv2d_282[0][0]                 \n",
      "                                                                 conv2d_284[0][0]                 \n",
      "                                                                 conv2d_286[0][0]                 \n",
      "                                                                 conv2d_288[0][0]                 \n",
      "                                                                 conv2d_290[0][0]                 \n",
      "                                                                 conv2d_292[0][0]                 \n",
      "                                                                 conv2d_294[0][0]                 \n",
      "                                                                 conv2d_296[0][0]                 \n",
      "                                                                 conv2d_298[0][0]                 \n",
      "                                                                 conv2d_300[0][0]                 \n",
      "                                                                 conv2d_302[0][0]                 \n",
      "                                                                 conv2d_304[0][0]                 \n",
      "                                                                 conv2d_306[0][0]                 \n",
      "                                                                 conv2d_308[0][0]                 \n",
      "                                                                 conv2d_310[0][0]                 \n",
      "                                                                 conv2d_312[0][0]                 \n",
      "                                                                 conv2d_314[0][0]                 \n",
      "                                                                 conv2d_316[0][0]                 \n",
      "                                                                 conv2d_318[0][0]                 \n",
      "                                                                 conv2d_320[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 1, 1, 1408)   5632        concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 1, 1, 1408)   0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1408)         0           activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 10)           14090       global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "prob (Activation)               (None, 10)           0           dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,070,986\n",
      "Trainable params: 5,995,018\n",
      "Non-trainable params: 75,968\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 162s 3ms/step - loss: 0.4914 - acc: 0.8239 - val_loss: 0.3646 - val_acc: 0.8630\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.3316 - acc: 0.8780 - val_loss: 0.3148 - val_acc: 0.8898\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.2977 - acc: 0.8919 - val_loss: 0.2840 - val_acc: 0.8966\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.2771 - acc: 0.8988 - val_loss: 0.3546 - val_acc: 0.8722\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.2506 - acc: 0.9081 - val_loss: 0.2634 - val_acc: 0.9035\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.2213 - acc: 0.9172 - val_loss: 0.2372 - val_acc: 0.9137\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.2065 - acc: 0.9214 - val_loss: 0.2497 - val_acc: 0.9124\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.1924 - acc: 0.9284 - val_loss: 0.2651 - val_acc: 0.9046\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.1753 - acc: 0.9341 - val_loss: 0.2699 - val_acc: 0.9051\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.1578 - acc: 0.9398 - val_loss: 0.2655 - val_acc: 0.9043\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 145s 3ms/step - loss: 0.1453 - acc: 0.9455 - val_loss: 0.2482 - val_acc: 0.9133\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.1288 - acc: 0.9517 - val_loss: 0.2862 - val_acc: 0.9077\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.1184 - acc: 0.9550 - val_loss: 0.2677 - val_acc: 0.9096\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.1076 - acc: 0.9585 - val_loss: 0.3115 - val_acc: 0.9068\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.1006 - acc: 0.9622 - val_loss: 0.3094 - val_acc: 0.9152\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0901 - acc: 0.9658 - val_loss: 0.2742 - val_acc: 0.9202\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0841 - acc: 0.9679 - val_loss: 0.2945 - val_acc: 0.9192\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0757 - acc: 0.9711 - val_loss: 0.3287 - val_acc: 0.9146\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0686 - acc: 0.9741 - val_loss: 0.3418 - val_acc: 0.9148\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0642 - acc: 0.9758 - val_loss: 0.3767 - val_acc: 0.9158\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0578 - acc: 0.9780 - val_loss: 0.3647 - val_acc: 0.9147\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0553 - acc: 0.9790 - val_loss: 0.3388 - val_acc: 0.9175\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0531 - acc: 0.9798 - val_loss: 0.4336 - val_acc: 0.9033\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0463 - acc: 0.9829 - val_loss: 0.4008 - val_acc: 0.9123\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0421 - acc: 0.9841 - val_loss: 0.3990 - val_acc: 0.9180\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0426 - acc: 0.9840 - val_loss: 0.3938 - val_acc: 0.9120\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0386 - acc: 0.9859 - val_loss: 0.3853 - val_acc: 0.9207\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0376 - acc: 0.9863 - val_loss: 0.3785 - val_acc: 0.9179\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.0365 - acc: 0.9875 - val_loss: 0.4210 - val_acc: 0.9130\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0320 - acc: 0.9883 - val_loss: 0.4033 - val_acc: 0.9140\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0348 - acc: 0.9884 - val_loss: 0.4106 - val_acc: 0.9162\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0307 - acc: 0.9891 - val_loss: 0.4145 - val_acc: 0.9175\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0307 - acc: 0.9887 - val_loss: 0.4224 - val_acc: 0.9176\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0285 - acc: 0.9899 - val_loss: 0.4364 - val_acc: 0.9146\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0273 - acc: 0.9902 - val_loss: 0.4565 - val_acc: 0.9138\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0268 - acc: 0.9904 - val_loss: 0.4717 - val_acc: 0.9075\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0285 - acc: 0.9895 - val_loss: 0.4637 - val_acc: 0.9133\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0265 - acc: 0.9911 - val_loss: 0.5444 - val_acc: 0.9095\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0230 - acc: 0.9923 - val_loss: 0.4746 - val_acc: 0.9132\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0247 - acc: 0.9915 - val_loss: 0.4521 - val_acc: 0.9157\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0232 - acc: 0.9918 - val_loss: 0.4485 - val_acc: 0.9168\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0232 - acc: 0.9919 - val_loss: 0.4414 - val_acc: 0.9198\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0205 - acc: 0.9929 - val_loss: 0.4496 - val_acc: 0.9204\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0219 - acc: 0.9921 - val_loss: 0.4344 - val_acc: 0.9193\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0205 - acc: 0.9922 - val_loss: 0.4788 - val_acc: 0.9171\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0177 - acc: 0.9940 - val_loss: 0.4751 - val_acc: 0.9144\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0230 - acc: 0.9920 - val_loss: 0.4671 - val_acc: 0.9183\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0171 - acc: 0.9942 - val_loss: 0.4896 - val_acc: 0.9183\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0201 - acc: 0.9932 - val_loss: 0.4591 - val_acc: 0.9163\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0183 - acc: 0.9935 - val_loss: 0.5221 - val_acc: 0.9136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x208016d00b8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9156"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_block(x, filters, n, strides, dropout):\n",
    "    x_res = Conv2D(filters, (3,3), strides=strides, padding=\"same\")(x)# , kernel_regularizer=l2(5e-4)\n",
    "    x_res = BatchNormalization()(x_res)\n",
    "    x_res = Activation('relu')(x_res)\n",
    "    x_res = Conv2D(filters, (3,3), padding=\"same\")(x_res)\n",
    "    # Alternative branch\n",
    "    x = Conv2D(filters, (1,1), strides=strides)(x)\n",
    "    # Merge Branches\n",
    "    x = Add()([x_res, x])\n",
    "\n",
    "    for i in range(n-1):\n",
    "        # Residual conection\n",
    "        x_res = BatchNormalization()(x)\n",
    "        x_res = Activation('relu')(x_res)\n",
    "        x_res = Conv2D(filters, (3,3), padding=\"same\")(x_res)\n",
    "        # Apply dropout if given\n",
    "        if dropout: x_res = Dropout(dropout)(x)\n",
    "        # Second part\n",
    "        x_res = BatchNormalization()(x_res)\n",
    "        x_res = Activation('relu')(x_res)\n",
    "        x_res = Conv2D(filters, (3,3), padding=\"same\")(x_res)\n",
    "        # Merge branches\n",
    "        x = Add()([x, x_res])\n",
    "\n",
    "    # Inter block part\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_dims, output_dim, n, k, dropout=None):\n",
    "    # Ensure n & k are correct\n",
    "    assert (n-4)%6 == 0\n",
    "    assert k%2 == 0\n",
    "    n = (n-4)//6 \n",
    "    \n",
    "    #1)Input\n",
    "    inputs = Input(shape=(input_dims))\n",
    "\n",
    "    #2)Initial part\n",
    "    x = Conv2D(16, (3,3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    #3)wide blocks (can be made into for loop)\n",
    "    x = main_block(x, 16*k, n, (1,1), dropout)\n",
    "    x = main_block(x, 32*k, n, (2,2), dropout)\n",
    "    x = main_block(x, 64*k, n, (2,2), dropout)\n",
    "\n",
    "    #4)Final part\n",
    "    x = AveragePooling2D((7,7))(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(output_dim, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model((28,28,1), 10, 40, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_325 (Conv2D)             (None, 28, 28, 16)   160         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 28, 28, 16)   64          conv2d_325[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 28, 28, 16)   0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_326 (Conv2D)             (None, 28, 28, 64)   9280        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 28, 28, 64)   256         conv2d_326[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 28, 28, 64)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_327 (Conv2D)             (None, 28, 28, 64)   36928       activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_328 (Conv2D)             (None, 28, 28, 64)   1088        activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 28, 28, 64)   0           conv2d_327[0][0]                 \n",
      "                                                                 conv2d_328[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 28, 28, 64)   256         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 28, 28, 64)   0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_329 (Conv2D)             (None, 28, 28, 64)   36928       activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 28, 28, 64)   256         conv2d_329[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 28, 28, 64)   0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_330 (Conv2D)             (None, 28, 28, 64)   36928       activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 28, 28, 64)   0           add_14[0][0]                     \n",
      "                                                                 conv2d_330[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 28, 28, 64)   256         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 28, 28, 64)   0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_331 (Conv2D)             (None, 28, 28, 64)   36928       activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 28, 28, 64)   256         conv2d_331[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 28, 28, 64)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_332 (Conv2D)             (None, 28, 28, 64)   36928       activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 28, 28, 64)   0           add_15[0][0]                     \n",
      "                                                                 conv2d_332[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 28, 28, 64)   256         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 28, 28, 64)   0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_333 (Conv2D)             (None, 28, 28, 64)   36928       activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 28, 28, 64)   256         conv2d_333[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 28, 28, 64)   0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_334 (Conv2D)             (None, 28, 28, 64)   36928       activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 28, 28, 64)   0           add_16[0][0]                     \n",
      "                                                                 conv2d_334[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 28, 28, 64)   256         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 28, 28, 64)   0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_335 (Conv2D)             (None, 28, 28, 64)   36928       activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 28, 28, 64)   256         conv2d_335[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 28, 28, 64)   0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_336 (Conv2D)             (None, 28, 28, 64)   36928       activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 28, 28, 64)   0           add_17[0][0]                     \n",
      "                                                                 conv2d_336[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 28, 28, 64)   256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 28, 28, 64)   0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_337 (Conv2D)             (None, 28, 28, 64)   36928       activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 28, 28, 64)   256         conv2d_337[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 28, 28, 64)   0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_338 (Conv2D)             (None, 28, 28, 64)   36928       activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 28, 28, 64)   0           add_18[0][0]                     \n",
      "                                                                 conv2d_338[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 28, 28, 64)   256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 28, 28, 64)   0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_339 (Conv2D)             (None, 14, 14, 128)  73856       activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 14, 14, 128)  512         conv2d_339[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 14, 14, 128)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_340 (Conv2D)             (None, 14, 14, 128)  147584      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_341 (Conv2D)             (None, 14, 14, 128)  8320        activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 14, 14, 128)  0           conv2d_340[0][0]                 \n",
      "                                                                 conv2d_341[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 14, 14, 128)  512         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 14, 14, 128)  0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_342 (Conv2D)             (None, 14, 14, 128)  147584      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 14, 14, 128)  512         conv2d_342[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 14, 14, 128)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 14, 14, 128)  147584      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 14, 14, 128)  0           add_20[0][0]                     \n",
      "                                                                 conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 14, 14, 128)  512         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 14, 14, 128)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 14, 14, 128)  147584      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 14, 14, 128)  512         conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 14, 14, 128)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 14, 14, 128)  147584      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 14, 14, 128)  0           add_21[0][0]                     \n",
      "                                                                 conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 14, 14, 128)  512         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 14, 14, 128)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 14, 14, 128)  147584      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 14, 14, 128)  512         conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 14, 14, 128)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 14, 14, 128)  147584      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 14, 14, 128)  0           add_22[0][0]                     \n",
      "                                                                 conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 14, 14, 128)  512         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 14, 14, 128)  0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 14, 14, 128)  147584      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 14, 14, 128)  512         conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 14, 14, 128)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 14, 14, 128)  147584      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 14, 14, 128)  0           add_23[0][0]                     \n",
      "                                                                 conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 14, 14, 128)  512         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 14, 14, 128)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 14, 14, 128)  147584      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 14, 14, 128)  512         conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 14, 14, 128)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 14, 14, 128)  147584      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 14, 14, 128)  0           add_24[0][0]                     \n",
      "                                                                 conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 14, 14, 128)  512         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 14, 14, 128)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 7, 7, 256)    295168      activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 7, 7, 256)    1024        conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 7, 7, 256)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 7, 7, 256)    590080      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 7, 7, 256)    33024       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 7, 7, 256)    0           conv2d_353[0][0]                 \n",
      "                                                                 conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 7, 7, 256)    1024        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 7, 7, 256)    0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 7, 7, 256)    590080      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 7, 7, 256)    1024        conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 7, 7, 256)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 7, 7, 256)    590080      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 7, 7, 256)    0           add_26[0][0]                     \n",
      "                                                                 conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 7, 7, 256)    1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 7, 7, 256)    0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 7, 7, 256)    590080      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 7, 7, 256)    1024        conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 7, 7, 256)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 7, 7, 256)    590080      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 7, 7, 256)    0           add_27[0][0]                     \n",
      "                                                                 conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 7, 7, 256)    1024        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 7, 7, 256)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 7, 7, 256)    590080      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 7, 7, 256)    1024        conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 7, 7, 256)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 7, 7, 256)    590080      activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 7, 7, 256)    0           add_28[0][0]                     \n",
      "                                                                 conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 7, 7, 256)    1024        add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 7, 7, 256)    0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 7, 7, 256)    590080      activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 7, 7, 256)    1024        conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 7, 7, 256)    0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 7, 7, 256)    590080      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 7, 7, 256)    0           add_29[0][0]                     \n",
      "                                                                 conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 7, 7, 256)    1024        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 7, 7, 256)    0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 7, 7, 256)    590080      activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 7, 7, 256)    1024        conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 7, 7, 256)    0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 7, 7, 256)    590080      activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 7, 7, 256)    0           add_30[0][0]                     \n",
      "                                                                 conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 7, 7, 256)    1024        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 7, 7, 256)    0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 1, 1, 256)    0           activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 256)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 10)           2570        flatten_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,965,546\n",
      "Trainable params: 8,954,762\n",
      "Non-trainable params: 10,784\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 198s 4ms/step - loss: 0.4928 - acc: 0.8188 - val_loss: 0.3255 - val_acc: 0.8856\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 186s 4ms/step - loss: 0.3068 - acc: 0.8892 - val_loss: 0.3173 - val_acc: 0.8858\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.2612 - acc: 0.9047 - val_loss: 0.2809 - val_acc: 0.8942\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.2319 - acc: 0.9160 - val_loss: 0.2725 - val_acc: 0.9012\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.2100 - acc: 0.9238 - val_loss: 0.2350 - val_acc: 0.9158\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.1919 - acc: 0.9310 - val_loss: 0.2381 - val_acc: 0.9123\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.1726 - acc: 0.9372 - val_loss: 0.2096 - val_acc: 0.9265\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.1516 - acc: 0.9450 - val_loss: 0.2093 - val_acc: 0.9274\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.1355 - acc: 0.9520 - val_loss: 0.1957 - val_acc: 0.9292\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.1164 - acc: 0.9579 - val_loss: 0.2161 - val_acc: 0.9289\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0981 - acc: 0.9650 - val_loss: 0.2248 - val_acc: 0.9251\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0825 - acc: 0.9705 - val_loss: 0.2451 - val_acc: 0.9280\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0679 - acc: 0.9752 - val_loss: 0.2416 - val_acc: 0.9299\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0537 - acc: 0.9814 - val_loss: 0.2546 - val_acc: 0.9330\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0468 - acc: 0.9835 - val_loss: 0.3035 - val_acc: 0.9221\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0412 - acc: 0.9847 - val_loss: 0.2798 - val_acc: 0.9317\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0338 - acc: 0.9881 - val_loss: 0.2793 - val_acc: 0.9330\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0297 - acc: 0.9897 - val_loss: 0.3025 - val_acc: 0.9290\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0269 - acc: 0.9900 - val_loss: 0.3213 - val_acc: 0.9280\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0266 - acc: 0.9907 - val_loss: 0.3264 - val_acc: 0.9273\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0230 - acc: 0.9922 - val_loss: 0.3640 - val_acc: 0.9242\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0220 - acc: 0.9919 - val_loss: 0.3938 - val_acc: 0.9191\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0189 - acc: 0.9931 - val_loss: 0.3377 - val_acc: 0.9316\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0174 - acc: 0.9939 - val_loss: 0.3509 - val_acc: 0.9333\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0182 - acc: 0.9935 - val_loss: 0.3935 - val_acc: 0.9228\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0197 - acc: 0.9937 - val_loss: 0.3436 - val_acc: 0.9327\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0164 - acc: 0.9942 - val_loss: 0.3641 - val_acc: 0.9327\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0166 - acc: 0.9943 - val_loss: 0.3991 - val_acc: 0.9247\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0128 - acc: 0.9956 - val_loss: 0.3886 - val_acc: 0.9285\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0138 - acc: 0.9951 - val_loss: 0.4385 - val_acc: 0.9250\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0115 - acc: 0.9961 - val_loss: 0.3881 - val_acc: 0.9318\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0160 - acc: 0.9944 - val_loss: 0.3362 - val_acc: 0.9321\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0113 - acc: 0.9960 - val_loss: 0.3671 - val_acc: 0.9302\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0112 - acc: 0.9965 - val_loss: 0.3661 - val_acc: 0.9278\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.3692 - val_acc: 0.9337\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.3731 - val_acc: 0.9295\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0133 - acc: 0.9952 - val_loss: 0.3770 - val_acc: 0.9317\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.3915 - val_acc: 0.9274\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0100 - acc: 0.9966 - val_loss: 0.3989 - val_acc: 0.9319\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0107 - acc: 0.9962 - val_loss: 0.3909 - val_acc: 0.9332\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0088 - acc: 0.9969 - val_loss: 0.4144 - val_acc: 0.9314\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.4092 - val_acc: 0.9325\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0100 - acc: 0.9968 - val_loss: 0.4208 - val_acc: 0.9308\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.3836 - val_acc: 0.9304\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0092 - acc: 0.9973 - val_loss: 0.4032 - val_acc: 0.9341\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.4213 - val_acc: 0.9317\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.4306 - val_acc: 0.9305\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.4601 - val_acc: 0.9265\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.4046 - val_acc: 0.9337\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 187s 4ms/step - loss: 0.0078 - acc: 0.9973 - val_loss: 0.4070 - val_acc: 0.9328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2084cbe8358>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x ,train_y, validation_data = (val_x, val_y), epochs = 50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
