{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vqlYOafJtUeC",
    "outputId": "84132e3b-9a39-4ba9-c667-5568d73e071e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## load the libraries \n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D, Flatten, Dropout, concatenate,GlobalAveragePooling2D, AveragePooling2D, Activation, Add, LeakyReLU, ReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform, Constant\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.optimizers import Optimizer\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Layer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVncBWJxuDlw"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/fashion/fashion-mnist_train.csv\")\n",
    "test = pd.read_csv(\"data/fashion/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YnGWC9_KuSiW"
   },
   "outputs": [],
   "source": [
    "train_x = train[list(train.columns)[1:]].values\n",
    "train_y = train['label'].values\n",
    "test_x = test[list(test.columns)[1:]].values\n",
    "test_y = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scAg7vEYuV13"
   },
   "outputs": [],
   "source": [
    "train_x=train_x.reshape(60000,28,28,1)\n",
    "test_x=test_x.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XWlPjipuXqe"
   },
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, num_classes = 10)\n",
    "test_y = to_categorical(test_y, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GGBCSc-uZF9"
   },
   "outputs": [],
   "source": [
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnOeUkcuubQX"
   },
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8P7r-n4ukkr"
   },
   "outputs": [],
   "source": [
    "class Swish(Layer):\n",
    "    def __init__(self, beta=1, **kwargs):\n",
    "        super(Swish, self).__init__(**kwargs)\n",
    "        self.beta = K.cast_to_floatx(beta)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K.sigmoid(self.beta * inputs) * inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'beta': float(self.beta)}\n",
    "        base_config = super(Swish, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7CXTX4Hu0Jz"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, array_to_img\n",
    "\n",
    "class MyImageDataGenerator(ImageDataGenerator):\n",
    "\n",
    "    def __init__(self, featurewise_center=False, samplewise_center=False,\n",
    "                 featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
    "                 zca_whitening=False, zca_epsilon=1e-06, rotation_range=0.0, width_shift_range=0.0,\n",
    "                 height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
    "                 channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False,\n",
    "                 vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0,\n",
    "                 random_crop=True, expand_rate=1.2):\n",
    "\n",
    "        self.my_rescale = rescale\n",
    "        if random_crop:\n",
    "            rescale = None\n",
    "        super().__init__(featurewise_center, samplewise_center, featurewise_std_normalization, samplewise_std_normalization, zca_whitening, zca_epsilon, rotation_range, width_shift_range,\n",
    "                         height_shift_range, brightness_range, shear_range, zoom_range, channel_shift_range, fill_mode, cval, horizontal_flip, vertical_flip, rescale, preprocessing_function, data_format, validation_split)\n",
    "        self.random_crop = random_crop\n",
    "        self.expand_rate = expand_rate\n",
    "\n",
    "    def scale_random_crop(self, original_img, seed):\n",
    "        np.random.seed(seed)\n",
    "        assert original_img.shape[2] == 1\n",
    "        dy, dx = original_img.shape[0:2]\n",
    "        expanded_img = img_to_array(array_to_img(original_img).resize((int(dy * self.expand_rate), int(dx * self.expand_rate))))\n",
    "        height, width = expanded_img.shape[0:2]\n",
    "\n",
    "        x = np.random.randint(0, width - dx + 1)\n",
    "        y = np.random.randint(0, height - dy + 1)\n",
    "        return expanded_img[y:(y + dy), x:(x + dx), :]\n",
    "\n",
    "    def flow(self, x, y=None, batch_size=32, shuffle=True, sample_weight=None,\n",
    "             seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None):\n",
    "        batches = super().flow(x=x, y=y, batch_size=batch_size, shuffle=shuffle, sample_weight=sample_weight,\n",
    "                               seed=seed, save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format, subset=subset)\n",
    "        while True:\n",
    "            batch = next(batches)\n",
    "            batch_x = batch[0]\n",
    "            batch_y = batch[1]\n",
    "            if self.random_crop:\n",
    "                x = np.zeros(batch_x.shape)\n",
    "                y = np.zeros(batch_y.shape)\n",
    "                for i in range(batch_x.shape[0]):\n",
    "                    x[i] = self.scale_random_crop(batch_x[i], seed)\n",
    "                    y[i] = batch_y[i]\n",
    "                batch_x = x * self.rescale if self.rescale is not None else x\n",
    "                batch_y = y\n",
    "            yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4w5sL12yCH_"
   },
   "outputs": [],
   "source": [
    "datagen = MyImageDataGenerator(\n",
    "        rotation_range=2,\n",
    "        zoom_range = 0.01,\n",
    "        width_shift_range=0.03,\n",
    "        height_shift_range=0.03,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False,\n",
    "        random_crop=True)\n",
    "\n",
    "\n",
    "datagen.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBq6QOPzvFWa"
   },
   "outputs": [],
   "source": [
    "def main_block(x, filters, n, strides, activation, initializer, dropout):\n",
    "    x_res = Conv2D(filters, (3,3), strides=strides, padding=\"same\", kernel_initializer=initializer, kernel_regularizer=l2(5e-4))(x)\n",
    "    x_res = BatchNormalization()(x_res)\n",
    "    x_res = activation()(x_res)\n",
    "    x_res = Conv2D(filters, (3,3), padding=\"same\", kernel_initializer=initializer)(x_res)\n",
    "    x = Conv2D(filters, (1,1), strides=strides)(x)\n",
    "    x = Add()([x_res, x])\n",
    "\n",
    "    for i in range(n-1):\n",
    "        x_res = BatchNormalization()(x)\n",
    "        x_res = activation()(x_res)\n",
    "        x_res = Conv2D(filters, (3,3), padding=\"same\", kernel_initializer=initializer)(x_res)\n",
    "        if dropout: x_res = Dropout(rate=dropout)(x)\n",
    "        x_res = BatchNormalization()(x_res)\n",
    "        x_res = activation()(x_res)\n",
    "        x_res = Conv2D(filters, (3,3), padding=\"same\", kernel_initializer=initializer)(x_res)\n",
    "        x = Add()([x, x_res])\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = activation()(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_dims, output_dim, n, k, activation, initializer, dropout=None):\n",
    "    assert (n-4)%6 == 0\n",
    "    assert k%2 == 0\n",
    "    n = (n-4)//6 \n",
    "    \n",
    "    inputs = Input(shape=(input_dims))\n",
    "\n",
    "    x = Conv2D(16, (3,3), padding=\"same\", kernel_initializer=initializer)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = activation()(x)\n",
    "\n",
    "    x = main_block(x, 32*k, n, (1,1), activation, initializer, dropout)\n",
    "    x = main_block(x, 48*k, n, (2,2), activation, initializer, dropout)\n",
    "    x = main_block(x, 64*k, n, (2,2), activation, initializer, dropout)\n",
    "\n",
    "    x = AveragePooling2D((7,7))(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(output_dim, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "akxReX-qvVL4",
    "outputId": "3d1d8fdb-66ea-4bb6-aeae-673030c9b8ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = build_model((28,28,1), 10, 40, 4, Swish, 'he_uniform',  0.1498182282337851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNXU0CoyvvTo"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Optimizer\n",
    "\n",
    "\n",
    "class AdaBound(Optimizer):\n",
    "\n",
    "    def __init__(self, lr=0.001, final_lr=0.1, beta_1=0.9, beta_2=0.999, gamma=1e-3,\n",
    "                 epsilon=None, decay=0., amsbound=False, weight_decay=0.0, **kwargs):\n",
    "        super(AdaBound, self).__init__(**kwargs)\n",
    "\n",
    "        if not 0. <= gamma <= 1.:\n",
    "            raise ValueError(\"Invalid `gamma` parameter. Must lie in [0, 1] range.\")\n",
    "\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "\n",
    "        self.final_lr = final_lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsbound = amsbound\n",
    "\n",
    "        self.weight_decay = float(weight_decay)\n",
    "        self.base_lr = float(lr)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        # Applies bounds on actual learning rate\n",
    "        step_size = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                          (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        final_lr = self.final_lr * lr / self.base_lr\n",
    "        lower_bound = final_lr * (1. - 1. / (self.gamma * t + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (self.gamma * t))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsbound:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            # apply weight decay\n",
    "            if self.weight_decay != 0.:\n",
    "                g += self.weight_decay * K.stop_gradient(p)\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            if self.amsbound:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                denom = (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                denom = (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            # Compute the bounds\n",
    "            step_size_p = step_size * K.ones_like(denom)\n",
    "            step_size_p_bound = step_size_p / denom\n",
    "            bounded_lr_t = m_t * K.minimum(K.maximum(step_size_p_bound,\n",
    "                                                     lower_bound), upper_bound)\n",
    "\n",
    "            p_t = p - bounded_lr_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'final_lr': float(self.final_lr),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'gamma': float(self.gamma),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'weight_decay': self.weight_decay,\n",
    "                  'amsbound': self.amsbound}\n",
    "        base_config = super(AdaBound, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqK2dsoYxjNt"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters for warmup and learning decay\n",
    "epochs=100\n",
    "warmup_epoch=epochs//20\n",
    "alpha=0\n",
    "\n",
    "#Linear scaling learning rate\n",
    "batch=32\n",
    "learning_rate=0.1*batch/256\n",
    "\n",
    "adabound=AdaBound(lr=learning_rate, final_lr=0.1, gamma=1e-03,weight_decay=0.,amsbound=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "syAsd0o52FQA",
    "outputId": "fed334eb-a259-47d1-94e2-e82d0ef68cee"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=adabound, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76IDQM4t0HWF"
   },
   "outputs": [],
   "source": [
    "def warmup_cosine_decay(epoch, epochs=epochs, lr=learning_rate, warmup_epoch=warmup_epoch, alpha=alpha):\n",
    "    return epoch*lr/warmup_epoch if epoch<warmup_epoch else (1 - alpha)*0.5*(1+np.cos(epoch*(np.pi))/epochs)+alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2eTW3tk1A81"
   },
   "outputs": [],
   "source": [
    "callbacks = [#LearningRateScheduler(warmup_cosine_decay),\n",
    "             ModelCheckpoint(\"best-sota-model2.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2d0n_ZWT02j2",
    "outputId": "e5a7015b-b727-4d9c-aea5-7cf4cfc8862b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "5062/5062 [==============================] - 877s 173ms/step - loss: 0.8195 - acc: 0.7755 - val_loss: 0.6832 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79014, saving model to best-sota-model2.hdf5\n",
      "Epoch 2/100\n",
      "5062/5062 [==============================] - 868s 171ms/step - loss: 0.4501 - acc: 0.8598 - val_loss: 0.4481 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.79014 to 0.85152, saving model to best-sota-model2.hdf5\n",
      "Epoch 3/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.3534 - acc: 0.8839 - val_loss: 0.4288 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85152 to 0.85812, saving model to best-sota-model2.hdf5\n",
      "Epoch 4/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.3068 - acc: 0.8989 - val_loss: 0.3546 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85812 to 0.88026, saving model to best-sota-model2.hdf5\n",
      "Epoch 5/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.2791 - acc: 0.9079 - val_loss: 0.3343 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.88026 to 0.89296, saving model to best-sota-model2.hdf5\n",
      "Epoch 6/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.2593 - acc: 0.9155 - val_loss: 0.2869 - val_acc: 0.9062\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.89296 to 0.90625, saving model to best-sota-model2.hdf5\n",
      "Epoch 7/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.2416 - acc: 0.9219 - val_loss: 0.2797 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.90625 to 0.90892, saving model to best-sota-model2.hdf5\n",
      "Epoch 8/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.2286 - acc: 0.9266 - val_loss: 0.2569 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.90892 to 0.91803, saving model to best-sota-model2.hdf5\n",
      "Epoch 9/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.2192 - acc: 0.9301 - val_loss: 0.2502 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91803\n",
      "Epoch 10/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.2080 - acc: 0.9336 - val_loss: 0.3124 - val_acc: 0.9019\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91803\n",
      "Epoch 11/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.2007 - acc: 0.9360 - val_loss: 0.3118 - val_acc: 0.9081\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.91803\n",
      "Epoch 12/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.1931 - acc: 0.9394 - val_loss: 0.2401 - val_acc: 0.9270\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.91803 to 0.92697, saving model to best-sota-model2.hdf5\n",
      "Epoch 13/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.1880 - acc: 0.9410 - val_loss: 0.2872 - val_acc: 0.9123\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92697\n",
      "Epoch 14/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.1808 - acc: 0.9439 - val_loss: 0.2603 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92697\n",
      "Epoch 15/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.1749 - acc: 0.9467 - val_loss: 0.2772 - val_acc: 0.9210\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92697\n",
      "Epoch 16/100\n",
      "5062/5062 [==============================] - 944s 186ms/step - loss: 0.1704 - acc: 0.9480 - val_loss: 0.2557 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92697\n",
      "Epoch 17/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.1657 - acc: 0.9506 - val_loss: 0.2594 - val_acc: 0.9268\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92697\n",
      "Epoch 18/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.1602 - acc: 0.9523 - val_loss: 0.2630 - val_acc: 0.9281\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.92697 to 0.92814, saving model to best-sota-model2.hdf5\n",
      "Epoch 19/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.1561 - acc: 0.9542 - val_loss: 0.2645 - val_acc: 0.9296\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.92814 to 0.92965, saving model to best-sota-model2.hdf5\n",
      "Epoch 20/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.1520 - acc: 0.9560 - val_loss: 0.2297 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.92965 to 0.93825, saving model to best-sota-model2.hdf5\n",
      "Epoch 21/100\n",
      "5062/5062 [==============================] - 879s 174ms/step - loss: 0.1461 - acc: 0.9587 - val_loss: 0.2309 - val_acc: 0.9385\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.93825 to 0.93850, saving model to best-sota-model2.hdf5\n",
      "Epoch 22/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.1432 - acc: 0.9599 - val_loss: 0.2446 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.93850\n",
      "Epoch 23/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.1386 - acc: 0.9615 - val_loss: 0.2720 - val_acc: 0.9295\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.93850\n",
      "Epoch 24/100\n",
      "5062/5062 [==============================] - 888s 175ms/step - loss: 0.1351 - acc: 0.9634 - val_loss: 0.2963 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.93850\n",
      "Epoch 25/100\n",
      "5062/5062 [==============================] - 882s 174ms/step - loss: 0.1324 - acc: 0.9646 - val_loss: 0.2379 - val_acc: 0.9424\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.93850 to 0.94243, saving model to best-sota-model2.hdf5\n",
      "Epoch 26/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.1287 - acc: 0.9659 - val_loss: 0.2522 - val_acc: 0.9337\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.94243\n",
      "Epoch 27/100\n",
      "5062/5062 [==============================] - 875s 173ms/step - loss: 0.1268 - acc: 0.9668 - val_loss: 0.2988 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.94243\n",
      "Epoch 28/100\n",
      "5062/5062 [==============================] - 866s 171ms/step - loss: 0.1211 - acc: 0.9693 - val_loss: 0.3275 - val_acc: 0.9262\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.94243\n",
      "Epoch 29/100\n",
      "5062/5062 [==============================] - 865s 171ms/step - loss: 0.1193 - acc: 0.9697 - val_loss: 0.2741 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.94243\n",
      "Epoch 30/100\n",
      "5062/5062 [==============================] - 865s 171ms/step - loss: 0.1147 - acc: 0.9720 - val_loss: 0.2585 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.94243\n",
      "Epoch 31/100\n",
      "5062/5062 [==============================] - 865s 171ms/step - loss: 0.1120 - acc: 0.9729 - val_loss: 0.2671 - val_acc: 0.9393\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.94243\n",
      "Epoch 32/100\n",
      "5062/5062 [==============================] - 865s 171ms/step - loss: 0.1101 - acc: 0.9741 - val_loss: 0.2750 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.94243\n",
      "Epoch 33/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.1084 - acc: 0.9742 - val_loss: 0.2835 - val_acc: 0.9371\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.94243\n",
      "Epoch 34/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.1043 - acc: 0.9762 - val_loss: 0.2967 - val_acc: 0.9348\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.94243\n",
      "Epoch 35/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.1022 - acc: 0.9763 - val_loss: 0.2820 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.94243\n",
      "Epoch 36/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.1007 - acc: 0.9771 - val_loss: 0.3152 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.94243\n",
      "Epoch 37/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0977 - acc: 0.9789 - val_loss: 0.2999 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.94243\n",
      "Epoch 38/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0953 - acc: 0.9794 - val_loss: 0.3074 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.94243\n",
      "Epoch 39/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0934 - acc: 0.9800 - val_loss: 0.3127 - val_acc: 0.9357\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.94243\n",
      "Epoch 40/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0914 - acc: 0.9810 - val_loss: 0.2884 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.94243\n",
      "Epoch 41/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0883 - acc: 0.9818 - val_loss: 0.3141 - val_acc: 0.9359\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.94243\n",
      "Epoch 42/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0873 - acc: 0.9825 - val_loss: 0.3071 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.94243\n",
      "Epoch 43/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0839 - acc: 0.9835 - val_loss: 0.3200 - val_acc: 0.9402\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.94243\n",
      "Epoch 44/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0840 - acc: 0.9835 - val_loss: 0.3222 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.94243\n",
      "Epoch 45/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0808 - acc: 0.9844 - val_loss: 0.3315 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.94243\n",
      "Epoch 46/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0812 - acc: 0.9841 - val_loss: 0.3478 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.94243\n",
      "Epoch 47/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0792 - acc: 0.9851 - val_loss: 0.3166 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.94243\n",
      "Epoch 48/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0772 - acc: 0.9854 - val_loss: 0.3347 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.94243\n",
      "Epoch 49/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0755 - acc: 0.9858 - val_loss: 0.3111 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.94243 to 0.94368, saving model to best-sota-model2.hdf5\n",
      "Epoch 50/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0742 - acc: 0.9865 - val_loss: 0.3351 - val_acc: 0.9374\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.94368\n",
      "Epoch 51/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0724 - acc: 0.9871 - val_loss: 0.3390 - val_acc: 0.9358\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.94368\n",
      "Epoch 52/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0722 - acc: 0.9870 - val_loss: 0.3640 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.94368\n",
      "Epoch 53/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0703 - acc: 0.9877 - val_loss: 0.3197 - val_acc: 0.9409\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.94368\n",
      "Epoch 54/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0682 - acc: 0.9881 - val_loss: 0.3386 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.94368\n",
      "Epoch 55/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0683 - acc: 0.9881 - val_loss: 0.3595 - val_acc: 0.9363\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.94368\n",
      "Epoch 56/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0679 - acc: 0.9884 - val_loss: 0.3238 - val_acc: 0.9404\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.94368\n",
      "Epoch 57/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0675 - acc: 0.9884 - val_loss: 0.3347 - val_acc: 0.9418\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.94368\n",
      "Epoch 58/100\n",
      "5062/5062 [==============================] - 873s 172ms/step - loss: 0.0643 - acc: 0.9894 - val_loss: 0.3373 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.94368\n",
      "Epoch 59/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.0644 - acc: 0.9892 - val_loss: 0.3230 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.94368\n",
      "Epoch 60/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0620 - acc: 0.9898 - val_loss: 0.3528 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.94368\n",
      "Epoch 61/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0620 - acc: 0.9899 - val_loss: 0.3632 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.94368\n",
      "Epoch 62/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0609 - acc: 0.9903 - val_loss: 0.3665 - val_acc: 0.9411\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.94368\n",
      "Epoch 63/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0598 - acc: 0.9904 - val_loss: 0.3637 - val_acc: 0.9389\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.94368\n",
      "Epoch 64/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0583 - acc: 0.9908 - val_loss: 0.3491 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.94368\n",
      "Epoch 65/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0589 - acc: 0.9906 - val_loss: 0.3358 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.94368 to 0.94460, saving model to best-sota-model2.hdf5\n",
      "Epoch 66/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0573 - acc: 0.9911 - val_loss: 0.3605 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.94460\n",
      "Epoch 67/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0547 - acc: 0.9918 - val_loss: 0.3406 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.94460 to 0.94485, saving model to best-sota-model2.hdf5\n",
      "Epoch 68/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0555 - acc: 0.9914 - val_loss: 0.3463 - val_acc: 0.9427\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.94485\n",
      "Epoch 69/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.0545 - acc: 0.9917 - val_loss: 0.3550 - val_acc: 0.9393\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.94485\n",
      "Epoch 70/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.0532 - acc: 0.9924 - val_loss: 0.3423 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.94485\n",
      "Epoch 71/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0510 - acc: 0.9927 - val_loss: 0.3340 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.94485 to 0.94820, saving model to best-sota-model2.hdf5\n",
      "Epoch 72/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0508 - acc: 0.9925 - val_loss: 0.3743 - val_acc: 0.9405\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.94820\n",
      "Epoch 73/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0522 - acc: 0.9925 - val_loss: 0.3575 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.94820\n",
      "Epoch 74/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0521 - acc: 0.9924 - val_loss: 0.3734 - val_acc: 0.9375\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.94820\n",
      "Epoch 75/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0493 - acc: 0.9931 - val_loss: 0.3857 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.94820\n",
      "Epoch 76/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0500 - acc: 0.9929 - val_loss: 0.3489 - val_acc: 0.9424\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.94820\n",
      "Epoch 77/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0486 - acc: 0.9929 - val_loss: 0.3863 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.94820\n",
      "Epoch 78/100\n",
      "5062/5062 [==============================] - 868s 171ms/step - loss: 0.0496 - acc: 0.9928 - val_loss: 0.3691 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.94820\n",
      "Epoch 79/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.0459 - acc: 0.9936 - val_loss: 0.3677 - val_acc: 0.9427\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.94820\n",
      "Epoch 80/100\n",
      "5062/5062 [==============================] - 868s 172ms/step - loss: 0.0462 - acc: 0.9934 - val_loss: 0.3872 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.94820\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0456 - acc: 0.9937 - val_loss: 0.3676 - val_acc: 0.9423\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.94820\n",
      "Epoch 82/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0451 - acc: 0.9938 - val_loss: 0.4072 - val_acc: 0.9396\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.94820\n",
      "Epoch 83/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.0447 - acc: 0.9941 - val_loss: 0.3745 - val_acc: 0.9407\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.94820\n",
      "Epoch 84/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.0466 - acc: 0.9934 - val_loss: 0.3359 - val_acc: 0.9443\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.94820\n",
      "Epoch 85/100\n",
      "5062/5062 [==============================] - 874s 173ms/step - loss: 0.0452 - acc: 0.9939 - val_loss: 0.3528 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.94820\n",
      "Epoch 86/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0413 - acc: 0.9947 - val_loss: 0.3860 - val_acc: 0.9387\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.94820\n",
      "Epoch 87/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0428 - acc: 0.9940 - val_loss: 0.3700 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.94820\n",
      "Epoch 88/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.0430 - acc: 0.9945 - val_loss: 0.3509 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.94820\n",
      "Epoch 89/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0442 - acc: 0.9940 - val_loss: 0.3789 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.94820\n",
      "Epoch 90/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0420 - acc: 0.9945 - val_loss: 0.3524 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.94820\n",
      "Epoch 91/100\n",
      "5062/5062 [==============================] - 870s 172ms/step - loss: 0.0413 - acc: 0.9949 - val_loss: 0.3508 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.94820\n",
      "Epoch 92/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0404 - acc: 0.9949 - val_loss: 0.4041 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.94820\n",
      "Epoch 93/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0411 - acc: 0.9945 - val_loss: 0.4273 - val_acc: 0.9373\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.94820\n",
      "Epoch 94/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0406 - acc: 0.9945 - val_loss: 0.3686 - val_acc: 0.9422\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.94820\n",
      "Epoch 95/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.0393 - acc: 0.9950 - val_loss: 0.3851 - val_acc: 0.9398\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.94820\n",
      "Epoch 96/100\n",
      "5062/5062 [==============================] - 873s 172ms/step - loss: 0.0380 - acc: 0.9951 - val_loss: 0.3876 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.94820\n",
      "Epoch 97/100\n",
      "5062/5062 [==============================] - 872s 172ms/step - loss: 0.0392 - acc: 0.9950 - val_loss: 0.3779 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.94820\n",
      "Epoch 98/100\n",
      "5062/5062 [==============================] - 871s 172ms/step - loss: 0.0375 - acc: 0.9955 - val_loss: 0.3735 - val_acc: 0.9438\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.94820\n",
      "Epoch 99/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0369 - acc: 0.9952 - val_loss: 0.3832 - val_acc: 0.9418\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.94820\n",
      "Epoch 100/100\n",
      "5062/5062 [==============================] - 869s 172ms/step - loss: 0.0376 - acc: 0.9951 - val_loss: 0.4097 - val_acc: 0.9422\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.94820\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(generator=datagen.flow(train_x, train_y, batch_size=batch),\n",
    "                    validation_data=datagen.flow(val_x, val_y, batch_size=batch),\n",
    "                    steps_per_epoch=train_x.shape[0] * 3 // batch, validation_steps=val_x.shape[0] * 2 // batch,\n",
    "                    epochs = epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkqjaXeW4UuJ"
   },
   "outputs": [],
   "source": [
    "model.save('current-sota-model2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HI4Rz2tH_4M1",
    "outputId": "db6b672f-5e81-4e7b-998f-843890513a93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9471"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSnboycmd7_R"
   },
   "outputs": [],
   "source": [
    "best_model = build_model((28,28,1), 10, 40, 4, Swish, 'he_uniform',  0.1498182282337851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxk58FIEd-vM"
   },
   "outputs": [],
   "source": [
    "best_model.load_weights(\"best-sota-model2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-rfOb_JZeHEB",
    "outputId": "7796d62a-41a1-4925-8ba5-ca592605c40e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9451"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pmeCAlNeHt4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WRN SOTA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
