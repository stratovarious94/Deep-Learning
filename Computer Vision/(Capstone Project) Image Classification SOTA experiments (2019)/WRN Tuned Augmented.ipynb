{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## load the libraries \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D, Flatten, Dropout, concatenate,GlobalAveragePooling2D, AveragePooling2D, ZeroPadding2D, add, Add, LeakyReLU, ReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.initializers import glorot_uniform, Constant\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Activation\n",
    "from keras.optimizers import Optimizer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from numpy import argmax, array_equal\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/fashion/fashion-mnist_train.csv\")\n",
    "test = pd.read_csv(\"data/fashion/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[list(train.columns)[1:]].values\n",
    "train_y = train['label'].values\n",
    "test_x = test[list(test.columns)[1:]].values\n",
    "test_y = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.reshape(60000,28,28,1)\n",
    "test_x=test_x.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, num_classes = 10)\n",
    "test_y = to_categorical(test_y, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x / 255\n",
    "test_x = test_x / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=2,\n",
    "        zoom_range = 0.01,\n",
    "        width_shift_range=0.03,\n",
    "        height_shift_range=0.03,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=False)\n",
    "\n",
    "\n",
    "datagen.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_block(x, filters, n, strides, activation, initializer, dropout):\n",
    "    # Normal part\n",
    "    x_res = Conv2D(filters, (3,3), strides=strides, padding=\"same\", kernel_initializer=initializer, kernel_regularizer=l2(5e-4))(x)\n",
    "    x_res = BatchNormalization()(x_res)\n",
    "    x_res = activation()(x_res)\n",
    "    x_res = Conv2D(filters, (3,3), padding=\"same\", kernel_initializer=initializer)(x_res)\n",
    "    # Alternative branch\n",
    "    x = Conv2D(filters, (1,1), strides=strides)(x)\n",
    "    # Merge Branches\n",
    "    x = Add()([x_res, x])\n",
    "\n",
    "    for i in range(n-1):\n",
    "        # Residual conection\n",
    "        x_res = BatchNormalization()(x)\n",
    "        x_res = activation()(x_res)\n",
    "        x_res = Conv2D(filters, (3,3), padding=\"same\", kernel_initializer=initializer)(x_res)\n",
    "        # Apply dropout if given\n",
    "        if dropout: x_res = Dropout(rate=dropout)(x)\n",
    "        # Second part\n",
    "        x_res = BatchNormalization()(x_res)\n",
    "        x_res = activation()(x_res)\n",
    "        x_res = Conv2D(filters, (3,3), padding=\"same\", kernel_initializer=initializer)(x_res)\n",
    "        # Merge branches\n",
    "        x = Add()([x, x_res])\n",
    "\n",
    "    # Inter block part\n",
    "    x = BatchNormalization()(x)\n",
    "    x = activation()(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_dims, output_dim, n, k, activation, initializer, dropout=None):\n",
    "    assert (n-4)%6 == 0\n",
    "    assert k%2 == 0\n",
    "    n = (n-4)//6 \n",
    "    \n",
    "    #1)Input\n",
    "    inputs = Input(shape=(input_dims))\n",
    "\n",
    "    #2)Initial part\n",
    "    x = Conv2D(16, (3,3), padding=\"same\", kernel_initializer=initializer)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = activation()(x)\n",
    "\n",
    "    #3)wide blocks (can be made into for loop)\n",
    "    x = main_block(x, 16*k, n, (1,1), activation, initializer, dropout)\n",
    "    x = main_block(x, 32*k, n, (2,2), activation, initializer, dropout)\n",
    "    x = main_block(x, 64*k, n, (2,2), activation, initializer, dropout)\n",
    "\n",
    "    #4)Final part\n",
    "    x = AveragePooling2D((7,7))(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(output_dim, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = build_model((28,28,1), 10, 40, 4, ReLU, 'he_uniform',  0.1498182282337851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 28, 28, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 28, 28, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 64)   9280        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 28, 28, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 64)   36928       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 64)   1088        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 64)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 28, 28, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 28, 28, 64)   256         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 28, 28, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 64)   36928       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 28, 28, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 28, 28, 64)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 28, 28, 64)   256         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 28, 28, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 28, 28, 64)   36928       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 64)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 28, 28, 64)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 28, 28, 64)   256         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 28, 28, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 28, 28, 64)   36928       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 64)   0           add_3[0][0]                      \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 28, 28, 64)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 28, 28, 64)   256         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 28, 28, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 64)   36928       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 64)   0           add_4[0][0]                      \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 28, 28, 64)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 28, 28, 64)   256         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 28, 28, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 28, 28, 64)   36928       re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 64)   0           add_5[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 28, 28, 64)   256         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 28, 28, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 14, 128)  73856       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 14, 14, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 14, 14, 128)  147584      re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 14, 14, 128)  8320        re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 128)  0           conv2d_16[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 14, 14, 128)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 14, 14, 128)  512         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 14, 14, 128)  147584      re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 128)  0           add_7[0][0]                      \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 14, 14, 128)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 14, 14, 128)  512         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 14, 14, 128)  147584      re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 128)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 14, 14, 128)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 14, 14, 128)  512         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 14, 14, 128)  147584      re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 128)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 14, 14, 128)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 14, 14, 128)  512         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 128)  147584      re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 128)  0           add_10[0][0]                     \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 14, 14, 128)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 14, 14, 128)  512         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 128)  147584      re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 128)  0           add_11[0][0]                     \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 14, 14, 128)  512         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 14, 14, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 7, 7, 256)    295168      re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 7, 7, 256)    1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 7, 7, 256)    590080      re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 7, 7, 256)    33024       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 256)    0           conv2d_29[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 7, 7, 256)    0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 7, 7, 256)    1024        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 7, 7, 256)    590080      re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 256)    0           add_13[0][0]                     \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 7, 7, 256)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 7, 7, 256)    1024        dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 7, 7, 256)    590080      re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 256)    0           add_14[0][0]                     \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 7, 7, 256)    0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 7, 7, 256)    1024        dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 7, 7, 256)    590080      re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 7, 7, 256)    0           add_15[0][0]                     \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 7, 7, 256)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 7, 7, 256)    1024        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 7, 7, 256)    590080      re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 7, 7, 256)    0           add_16[0][0]                     \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 7, 7, 256)    0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 7, 7, 256)    1024        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 7, 7, 256)    590080      re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 7, 7, 256)    0           add_17[0][0]                     \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 7, 7, 256)    1024        add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 7, 7, 256)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,083,626\n",
      "Trainable params: 5,077,322\n",
      "Non-trainable params: 6,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5,min_lr=0.00001),\n",
    "             ModelCheckpoint(\"best-augmented-model.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(0.00011056751621948101), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Infernal\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "5062/5062 [==============================] - 428s 85ms/step - loss: 0.7274 - acc: 0.8557 - val_loss: 0.5702 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.88877, saving model to best-augmented-model.hdf5\n",
      "Epoch 2/100\n",
      "5062/5062 [==============================] - 423s 83ms/step - loss: 0.4329 - acc: 0.9112 - val_loss: 0.4019 - val_acc: 0.9116\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.88877 to 0.91160, saving model to best-augmented-model.hdf5\n",
      "Epoch 3/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.3303 - acc: 0.9241 - val_loss: 0.3669 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91160\n",
      "Epoch 4/100\n",
      "5062/5062 [==============================] - 423s 83ms/step - loss: 0.2730 - acc: 0.9324 - val_loss: 0.2960 - val_acc: 0.9245\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91160 to 0.92447, saving model to best-augmented-model.hdf5\n",
      "Epoch 5/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.2384 - acc: 0.9390 - val_loss: 0.2856 - val_acc: 0.9240\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92447\n",
      "Epoch 6/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.2126 - acc: 0.9442 - val_loss: 0.3955 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92447\n",
      "Epoch 7/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1937 - acc: 0.9479 - val_loss: 0.2760 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.92447 to 0.92605, saving model to best-augmented-model.hdf5\n",
      "Epoch 8/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1788 - acc: 0.9524 - val_loss: 0.2676 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.92605 to 0.92839, saving model to best-augmented-model.hdf5\n",
      "Epoch 9/100\n",
      "5062/5062 [==============================] - 423s 83ms/step - loss: 0.1645 - acc: 0.9557 - val_loss: 0.2576 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.92839 to 0.93107, saving model to best-augmented-model.hdf5\n",
      "Epoch 10/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1548 - acc: 0.9586 - val_loss: 0.3229 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.93107\n",
      "Epoch 11/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1441 - acc: 0.9616 - val_loss: 0.2927 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.93107\n",
      "Epoch 12/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1346 - acc: 0.9651 - val_loss: 0.2411 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.93107 to 0.93783, saving model to best-augmented-model.hdf5\n",
      "Epoch 13/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1271 - acc: 0.9674 - val_loss: 0.2511 - val_acc: 0.9367\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.93783\n",
      "Epoch 14/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1202 - acc: 0.9694 - val_loss: 0.2619 - val_acc: 0.9361\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.93783\n",
      "Epoch 15/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.1128 - acc: 0.9718 - val_loss: 0.3289 - val_acc: 0.9179\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 5.5283759138546884e-05.\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.93783\n",
      "Epoch 16/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.0853 - acc: 0.9820 - val_loss: 0.2734 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.93783 to 0.93875, saving model to best-augmented-model.hdf5\n",
      "Epoch 17/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.0762 - acc: 0.9842 - val_loss: 0.2857 - val_acc: 0.9399\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.93875 to 0.93992, saving model to best-augmented-model.hdf5\n",
      "Epoch 18/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.0694 - acc: 0.9860 - val_loss: 0.2740 - val_acc: 0.9404\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.93992 to 0.94042, saving model to best-augmented-model.hdf5\n",
      "Epoch 19/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0666 - acc: 0.9865 - val_loss: 0.2749 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.94042 to 0.94276, saving model to best-augmented-model.hdf5\n",
      "Epoch 20/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.0630 - acc: 0.9874 - val_loss: 0.2817 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.94276\n",
      "Epoch 21/100\n",
      "5062/5062 [==============================] - 423s 84ms/step - loss: 0.0603 - acc: 0.9883 - val_loss: 0.2825 - val_acc: 0.9413\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.94276\n",
      "Epoch 22/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0592 - acc: 0.9885 - val_loss: 0.2893 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 2.7641879569273442e-05.\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.94276\n",
      "Epoch 23/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0460 - acc: 0.9934 - val_loss: 0.2885 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.94276 to 0.94677, saving model to best-augmented-model.hdf5\n",
      "Epoch 24/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0428 - acc: 0.9940 - val_loss: 0.2842 - val_acc: 0.9478\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.94677 to 0.94778, saving model to best-augmented-model.hdf5\n",
      "Epoch 25/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0404 - acc: 0.9942 - val_loss: 0.3135 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.94778\n",
      "Epoch 26/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0386 - acc: 0.9947 - val_loss: 0.2960 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.94778\n",
      "Epoch 27/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0367 - acc: 0.9951 - val_loss: 0.2906 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.3820939784636721e-05.\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.94778\n",
      "Epoch 28/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0318 - acc: 0.9968 - val_loss: 0.2952 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.94778\n",
      "Epoch 29/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0306 - acc: 0.9971 - val_loss: 0.3041 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.94778\n",
      "Epoch 30/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0294 - acc: 0.9973 - val_loss: 0.3153 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.94778\n",
      "Epoch 31/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0274 - acc: 0.9978 - val_loss: 0.3166 - val_acc: 0.9453\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.94778\n",
      "Epoch 32/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0265 - acc: 0.9981 - val_loss: 0.3308 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.94778\n",
      "Epoch 33/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0256 - acc: 0.9980 - val_loss: 0.3180 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.94778\n",
      "Epoch 34/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0251 - acc: 0.9982 - val_loss: 0.3265 - val_acc: 0.9428\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.94778\n",
      "Epoch 35/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0246 - acc: 0.9982 - val_loss: 0.3272 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.94778\n",
      "Epoch 36/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0241 - acc: 0.9981 - val_loss: 0.3123 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.94778 to 0.94811, saving model to best-augmented-model.hdf5\n",
      "Epoch 37/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0236 - acc: 0.9982 - val_loss: 0.3154 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.94811\n",
      "Epoch 38/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0229 - acc: 0.9984 - val_loss: 0.3122 - val_acc: 0.9465\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.94811\n",
      "Epoch 39/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0227 - acc: 0.9983 - val_loss: 0.3308 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.94811\n",
      "Epoch 40/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0219 - acc: 0.9984 - val_loss: 0.3468 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.94811\n",
      "Epoch 41/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0220 - acc: 0.9983 - val_loss: 0.3350 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.94811\n",
      "Epoch 42/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0212 - acc: 0.9984 - val_loss: 0.3293 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.94811\n",
      "Epoch 43/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0211 - acc: 0.9984 - val_loss: 0.3252 - val_acc: 0.9476\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.94811\n",
      "Epoch 44/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0205 - acc: 0.9985 - val_loss: 0.3371 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.94811\n",
      "Epoch 45/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0203 - acc: 0.9985 - val_loss: 0.3368 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.94811\n",
      "Epoch 46/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0201 - acc: 0.9985 - val_loss: 0.3460 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.94811\n",
      "Epoch 47/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0198 - acc: 0.9984 - val_loss: 0.3326 - val_acc: 0.9473\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.94811\n",
      "Epoch 48/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0195 - acc: 0.9985 - val_loss: 0.3340 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.94811\n",
      "Epoch 49/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0194 - acc: 0.9986 - val_loss: 0.3353 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.94811\n",
      "Epoch 50/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0190 - acc: 0.9985 - val_loss: 0.3207 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.94811\n",
      "Epoch 51/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0183 - acc: 0.9988 - val_loss: 0.3334 - val_acc: 0.9457\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.94811\n",
      "Epoch 52/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0183 - acc: 0.9988 - val_loss: 0.3500 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.94811\n",
      "Epoch 53/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0181 - acc: 0.9988 - val_loss: 0.3326 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.94811\n",
      "Epoch 54/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0181 - acc: 0.9986 - val_loss: 0.3180 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.94811\n",
      "Epoch 55/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0177 - acc: 0.9987 - val_loss: 0.3378 - val_acc: 0.9464\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.94811\n",
      "Epoch 56/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0175 - acc: 0.9987 - val_loss: 0.3386 - val_acc: 0.9472\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.94811\n",
      "Epoch 57/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0175 - acc: 0.9987 - val_loss: 0.3221 - val_acc: 0.9505\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.94811 to 0.95045, saving model to best-augmented-model.hdf5\n",
      "Epoch 58/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0169 - acc: 0.9988 - val_loss: 0.3312 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.95045\n",
      "Epoch 59/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0168 - acc: 0.9987 - val_loss: 0.3514 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.95045\n",
      "Epoch 60/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0168 - acc: 0.9988 - val_loss: 0.3457 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.95045\n",
      "Epoch 61/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0169 - acc: 0.9987 - val_loss: 0.3241 - val_acc: 0.9495\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.95045\n",
      "Epoch 62/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0165 - acc: 0.9987 - val_loss: 0.3523 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.95045\n",
      "Epoch 63/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0164 - acc: 0.9987 - val_loss: 0.3338 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.95045\n",
      "Epoch 64/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0162 - acc: 0.9987 - val_loss: 0.3371 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.95045\n",
      "Epoch 65/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0160 - acc: 0.9988 - val_loss: 0.3407 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.95045\n",
      "Epoch 66/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0158 - acc: 0.9987 - val_loss: 0.3463 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.95045\n",
      "Epoch 67/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0159 - acc: 0.9987 - val_loss: 0.3734 - val_acc: 0.9429\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.95045\n",
      "Epoch 68/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0157 - acc: 0.9987 - val_loss: 0.3442 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.95045\n",
      "Epoch 69/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0153 - acc: 0.9988 - val_loss: 0.3645 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.95045\n",
      "Epoch 70/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0149 - acc: 0.9989 - val_loss: 0.3497 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.95045\n",
      "Epoch 71/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0151 - acc: 0.9988 - val_loss: 0.3469 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.95045\n",
      "Epoch 72/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0148 - acc: 0.9989 - val_loss: 0.3661 - val_acc: 0.9431\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.95045\n",
      "Epoch 73/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0149 - acc: 0.9987 - val_loss: 0.3471 - val_acc: 0.9479\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.95045\n",
      "Epoch 74/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0147 - acc: 0.9988 - val_loss: 0.3494 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.95045\n",
      "Epoch 75/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0149 - acc: 0.9987 - val_loss: 0.3505 - val_acc: 0.9460\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.95045\n",
      "Epoch 76/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0147 - acc: 0.9987 - val_loss: 0.3770 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.95045\n",
      "Epoch 77/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0145 - acc: 0.9987 - val_loss: 0.3399 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.95045\n",
      "Epoch 78/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0142 - acc: 0.9989 - val_loss: 0.3574 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.95045\n",
      "Epoch 79/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0143 - acc: 0.9989 - val_loss: 0.3542 - val_acc: 0.9461\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.95045\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0141 - acc: 0.9988 - val_loss: 0.3620 - val_acc: 0.9466\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.95045\n",
      "Epoch 81/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0140 - acc: 0.9988 - val_loss: 0.3515 - val_acc: 0.9470\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.95045\n",
      "Epoch 82/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0140 - acc: 0.9987 - val_loss: 0.3504 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.95045\n",
      "Epoch 83/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0138 - acc: 0.9990 - val_loss: 0.3383 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.95045\n",
      "Epoch 84/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0138 - acc: 0.9989 - val_loss: 0.3513 - val_acc: 0.9471\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.95045\n",
      "Epoch 85/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0136 - acc: 0.9989 - val_loss: 0.3500 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.95045\n",
      "Epoch 86/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0135 - acc: 0.9989 - val_loss: 0.3541 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.95045\n",
      "Epoch 87/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0131 - acc: 0.9990 - val_loss: 0.3658 - val_acc: 0.9474\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.95045\n",
      "Epoch 88/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0134 - acc: 0.9989 - val_loss: 0.3572 - val_acc: 0.9462\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.95045\n",
      "Epoch 89/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0133 - acc: 0.9989 - val_loss: 0.3491 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.95045\n",
      "Epoch 90/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0132 - acc: 0.9989 - val_loss: 0.3489 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.95045\n",
      "Epoch 91/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0133 - acc: 0.9988 - val_loss: 0.3535 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.95045\n",
      "Epoch 92/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0131 - acc: 0.9989 - val_loss: 0.3532 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.95045\n",
      "Epoch 93/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0128 - acc: 0.9990 - val_loss: 0.3635 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.95045\n",
      "Epoch 94/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0132 - acc: 0.9988 - val_loss: 0.3524 - val_acc: 0.9469\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.95045\n",
      "Epoch 95/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0126 - acc: 0.9990 - val_loss: 0.3519 - val_acc: 0.9467\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.95045\n",
      "Epoch 96/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0132 - acc: 0.9988 - val_loss: 0.3571 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.95045\n",
      "Epoch 97/100\n",
      "5062/5062 [==============================] - 424s 84ms/step - loss: 0.0131 - acc: 0.9987 - val_loss: 0.3655 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.95045\n",
      "Epoch 98/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0126 - acc: 0.9989 - val_loss: 0.3625 - val_acc: 0.9451\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.95045\n",
      "Epoch 99/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0124 - acc: 0.9990 - val_loss: 0.3744 - val_acc: 0.9442\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.95045\n",
      "Epoch 100/100\n",
      "5062/5062 [==============================] - 425s 84ms/step - loss: 0.0122 - acc: 0.9990 - val_loss: 0.3457 - val_acc: 0.9483\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.95045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab66d2cef0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=datagen.flow(train_x, train_y, batch_size=32),\n",
    "                    validation_data=datagen.flow(val_x, val_y, batch_size=32), \n",
    "                    validation_steps=val_x.shape[0] * 2 // 32, steps_per_epoch=train_x.shape[0] * 3 // 32, \n",
    "                    epochs = 100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9527"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model((28,28,1), 10, 40, 4, ReLU, 'he_uniform',  0.1498182282337851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.load_weights(\"best-augmented-model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9524"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_model.predict(test_x)\n",
    "accuracy_score(np.argmax(test_y, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
