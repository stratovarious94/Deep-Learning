{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Exercise 4\n",
    "\n",
    "Daskalopoulos Ioannis (f3351805)<br>\n",
    "Ntouskas Fotios (f3351813)<br>\n",
    "Palassopoulos Vasileios (f3351814)<br>\n",
    "Spantouri Natalia (f3351817)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, TweetTokenizer #Tokenizing sentences\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"europarl-v7-en.txt\", encoding = \"utf8\") as file:\n",
    "    source = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split source into training / development / test set - 70/10/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sents = sent_tokenize(source)\n",
    "sents_len=len(source_sents)\n",
    "train_sents=source_sents[0:round(sents_len*0.7)]\n",
    "development_sents=source_sents[round(sents_len*0.7):round(sents_len*0.8)]\n",
    "test_sents=source_sents[round(sents_len*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_wt = TweetTokenizer()\n",
    "\n",
    "train_tokens = tweet_wt.tokenize(' '.join(train_sents))\n",
    "development_tokens = tweet_wt.tokenize(' '.join(development_sents))\n",
    "test_tokens = tweet_wt.tokenize(' '.join(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding tokens appearing in the training test rarely... Replacing those with \\*UKN\\* in all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(train_tokens)\n",
    "frequent_tokens=set([k for k,v in count.items() if v >= 10])\n",
    "train_tokens=[train_token if (train_token in frequent_tokens) else \"*UNK*\" for train_token in train_tokens]\n",
    "development_tokens=[development_token if (development_token in frequent_tokens) else \"*UNK*\" for development_token in development_tokens]\n",
    "test_tokens=[test_token if (test_token in frequent_tokens) else \"*UNK*\" for test_token in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize sets by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_tokenized = []\n",
    "development_sents_tokenized = []\n",
    "test_sents_tokenized = []\n",
    "\n",
    "for sent in train_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    train_sents_tokenized.append(sent)\n",
    "for sent in development_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    development_sents_tokenized.append(sent)\n",
    "for sent in test_sents:\n",
    "    sent_tmp = tweet_wt.tokenize(sent)\n",
    "    sent = [word if (word in frequent_tokens) else \"*UNK*\" for word in sent_tmp]\n",
    "    test_sents_tokenized.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tokens into Bigram and Trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in train_sents_tokenized:\n",
    "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True, left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
    "    \n",
    "unigram_counter[('*start*',)] = len(train_sents_tokenized)\n",
    "bigram_counter[('*start*','*start*')] = len(train_sents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring ngram methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(train_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_bigram_prob(sent, idx, alpha, vocab_size):\n",
    "    return math.log2((bigram_counter[(sent[idx-1], sent[idx])] + round(alpha,4)) / (unigram_counter[(sent[idx-1],)] + round(alpha,4)*vocab_size))\n",
    "\n",
    "def log_trigram_prob(sent, idx, alpha, vocab_size):\n",
    "    return math.log2((trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size))\n",
    "\n",
    "def bigram(sents_tokenized):\n",
    "    bigram_cnt = 0\n",
    "    sum_prob = 0    \n",
    "    for sent in sents_tokenized:\n",
    "        sent = ['*start*'] + sent + ['*end*']\n",
    "        for idx in range(1,len(sent)):\n",
    "            sum_prob += log_bigram_prob(sent, idx, alpha, vocab_size)\n",
    "            bigram_cnt+=1\n",
    "    return sum_prob, bigram_cnt\n",
    "\n",
    "def trigram(sents_tokenized, alpha):\n",
    "    trigram_cnt = 0\n",
    "    sum_prob = 0    \n",
    "    for sent in sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            sum_prob += log_trigram_prob(sent, idx, alpha, vocab_size)                                                   \n",
    "            trigram_cnt+=1\n",
    "    return sum_prob, trigram_cnt\n",
    "            \n",
    "def best_alpha(perpl,lowest_perplexity,lowest_alpha):\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_alpha=round(alpha,4)\n",
    "    return lowest_perplexity, lowest_alpha\n",
    "\n",
    "def HC_entropy(sum_prob, ngram_cnt):\n",
    "    HC = -sum_prob / ngram_cnt\n",
    "    perpl = math.pow(2,HC)\n",
    "    return HC, perpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the bigram model is:  0.006\n"
     ]
    }
   ],
   "source": [
    "lowest_bigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.005,0.025,0.0005):\n",
    "    sum_prob, bigram_cnt = bigram(development_sents_tokenized)\n",
    "    HC, perpl = HC_entropy(sum_prob, bigram_cnt)\n",
    "    lowest_perplexity, lowest_bigram_alpha = best_alpha(perpl, lowest_perplexity, lowest_bigram_alpha)\n",
    "    \n",
    "print(\"The alpha that produces the lowest perplexity for the bigram model is: \", lowest_bigram_alpha)\n",
    "b_alpha=lowest_bigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning alpha wrt perplexity for the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alpha that produces the lowest perplexity for the trigram model is:  0.002\n"
     ]
    }
   ],
   "source": [
    "lowest_trigram_alpha=1\n",
    "lowest_perplexity=100000\n",
    "for alpha in np.arange(0.001,0.01,0.0005):\n",
    "    sum_prob, trigram_cnt = trigram(development_sents_tokenized, alpha)\n",
    "    HC, perpl = HC_entropy(sum_prob, bigram_cnt)\n",
    "    lowest_perplexity, lowest_trigram_alpha = best_alpha(perpl, lowest_perplexity, lowest_trigram_alpha)\n",
    "\n",
    "print(\"The alpha that produces the lowest perplexity for the trigram model is: \", lowest_trigram_alpha)\n",
    "t_alpha=lowest_trigram_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition for computing probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sentences(sents_tokenized):\n",
    "    probabilities_list = []\n",
    "    sentences_list = []\n",
    "    for sent in sents_tokenized:\n",
    "        sum_prob = 0\n",
    "        sent = ['*start*'] + sent + ['*end*']\n",
    "        for idx in range(1,len(sent)):\n",
    "            sum_prob += log_bigram_prob(sent, idx, b_alpha, vocab_size)\n",
    "        probabilities_list.append(sum_prob)\n",
    "        sentences_list.append(' '.join(sent[1:len(sent)-2]))\n",
    "    return probabilities_list, sentences_list\n",
    "\n",
    "def trigram_sentences(sents_tokenized):\n",
    "    probabilities_list = []\n",
    "    sentences_list = []\n",
    "    for sent in sents_tokenized:\n",
    "        sum_prob = 0\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            sum_prob += log_trigram_prob(sent, idx, t_alpha, vocab_size)\n",
    "        probabilities_list.append(sum_prob)\n",
    "        sentences_list.append(' '.join(sent[2:len(sent)-3]))\n",
    "    return probabilities_list, sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals</th>\n",
       "      <td>-265.154277</td>\n",
       "      <td>6.997787e-116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment</th>\n",
       "      <td>-242.691735</td>\n",
       "      <td>3.983994e-106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe</th>\n",
       "      <td>-256.218822</td>\n",
       "      <td>5.315933e-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e</th>\n",
       "      <td>-349.601468</td>\n",
       "      <td>1.479147e-152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the USA and Japan</th>\n",
       "      <td>-38.140553</td>\n",
       "      <td>2.727522e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       "In the agricultural sector , rapid restructurin...  -265.154277   \n",
       "Furthermore , the liberalisation of trade and t...  -242.691735   \n",
       "Agricultural production in Greece - and elsewhe...  -256.218822   \n",
       "We believe that , rather than defending the int...  -349.601468   \n",
       "the USA and Japan                                    -38.140553   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       "In the agricultural sector , rapid restructurin...    6.997787e-116  \n",
       "Furthermore , the liberalisation of trade and t...    3.983994e-106  \n",
       "Agricultural production in Greece - and elsewhe...    5.315933e-112  \n",
       "We believe that , rather than defending the int...    1.479147e-152  \n",
       "the USA and Japan                                      2.727522e-17  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_list, sentences_list = bigram_sentences(test_sents_tokenized)\n",
    "df_sent_bigram = pd.DataFrame({'Sentence': sentences_list, 'Probability': probabilities_list, 'Log-Probability': np.exp(probabilities_list)})\n",
    "df_sent_bigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences:\n",
      "Mean Probability: -196.81242769991334\n",
      "Mean Log Probability: 7.039408974668455e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences:\")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_list))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the Test set - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In the agricultural sector , rapid restructuring is being called for in order to concentrate the land in a few hands and forge another link in the chain controlled by the network of multinationals</th>\n",
       "      <td>-339.229816</td>\n",
       "      <td>4.724575e-148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Furthermore , the liberalisation of trade and the abolition of duty and subsidies have hit agricultural production directly , reducing farmers ' incomes , *UNK* farming and increasing unemployment</th>\n",
       "      <td>-280.635642</td>\n",
       "      <td>1.322785e-122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agricultural production in Greece - and elsewhere - is being sacrificed in order to protect and corner a larger share of the international market for processed products from central and northern Europe</th>\n",
       "      <td>-334.407844</td>\n",
       "      <td>5.868385e-146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>We believe that , rather than defending the interests of the people , the EU will again endeavour at the new round of talks to stake a bigger claim for the European monopolies , in competition with the other imperialist centres , i . e</th>\n",
       "      <td>-397.242448</td>\n",
       "      <td>3.018539e-173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the USA and Japan</th>\n",
       "      <td>-36.959012</td>\n",
       "      <td>8.890066e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       "In the agricultural sector , rapid restructurin...  -339.229816   \n",
       "Furthermore , the liberalisation of trade and t...  -280.635642   \n",
       "Agricultural production in Greece - and elsewhe...  -334.407844   \n",
       "We believe that , rather than defending the int...  -397.242448   \n",
       "the USA and Japan                                    -36.959012   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       "In the agricultural sector , rapid restructurin...    4.724575e-148  \n",
       "Furthermore , the liberalisation of trade and t...    1.322785e-122  \n",
       "Agricultural production in Greece - and elsewhe...    5.868385e-146  \n",
       "We believe that , rather than defending the int...    3.018539e-173  \n",
       "the USA and Japan                                      8.890066e-17  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_list, sentences_list = trigram_sentences(test_sents_tokenized)\n",
    "df_sent_trigram = pd.DataFrame({'Sentence': sentences_list, 'Probability': probabilities_list, 'Log-Probability': np.exp(probabilities_list)})\n",
    "df_sent_trigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences:\n",
      "Mean Probability: -231.17038543365476\n",
      "Mean Log Probability: 8.740469742892291e-07\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences:\")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_list))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_text(sents_tokenized):\n",
    "    text = []\n",
    "    for sent in sents_tokenized:\n",
    "        rand_sent = []\n",
    "        for i in range(len(sent)):\n",
    "            rand_sent.append(test_tokens[random.randint(0, len(test_tokens)-1)])\n",
    "        text.append(rand_sent)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>, to that to One , to ' with a . in a guarantee dossiers The fast the and such in industry to . should , this the . they Nice *UNK* the ,</th>\n",
       "      <td>-436.158183</td>\n",
       "      <td>3.792346e-190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coming the a the list of not . measures not the abuses , in reply livestock major to , has the know *UNK* enormous legislature The has the ,</th>\n",
       "      <td>-414.462459</td>\n",
       "      <td>1.002862e-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>countries is mean rapporteur's not The Geneva quite is want effects I attention Regarding Secondly . . . European FR with will plans combat of right might - Please year term is</th>\n",
       "      <td>-590.511278</td>\n",
       "      <td>3.501149e-257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment there that reaction and , Commission the Commission , resolution are , Institute directive will by obstruct letter the proposal s hold . the , to has of least , European also the President the thank potential open In problems much investing this the</th>\n",
       "      <td>-604.934506</td>\n",
       "      <td>1.906700e-263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 Europe ] quite</th>\n",
       "      <td>-100.445938</td>\n",
       "      <td>2.381680e-44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       ", to that to One , to ' with a . in a guarantee...  -436.158183   \n",
       "coming the a the list of not . measures not the...  -414.462459   \n",
       "countries is mean rapporteur's not The Geneva q...  -590.511278   \n",
       "treatment there that reaction and , Commission ...  -604.934506   \n",
       "3 Europe ] quite                                    -100.445938   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       ", to that to One , to ' with a . in a guarantee...    3.792346e-190  \n",
       "coming the a the list of not . measures not the...    1.002862e-180  \n",
       "countries is mean rapporteur's not The Geneva q...    3.501149e-257  \n",
       "treatment there that reaction and , Commission ...    1.906700e-263  \n",
       "3 Europe ] quite                                       2.381680e-44  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_random, sentences_random = bigram_sentences(create_random_text(test_sents_tokenized))\n",
    "df_sent_rand_bigram = pd.DataFrame({'Sentence': sentences_random, 'Probability': probabilities_random, 'Log-Probability': np.exp(probabilities_random)})\n",
    "df_sent_rand_bigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -417.3817186157106\n",
      "Mean Log Probability: 5.367602426836255e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_random))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probabilities for the random sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "      <th>Log-Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>. Commission . . see and able have , . on common contents constraints way the I follow-up three discussed the that chain policies , 11.30 by with the to support , to is</th>\n",
       "      <td>-509.951100</td>\n",
       "      <td>3.396652e-222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in are the the wish The flexibility and financial for , will Conference the EU's all time I wing Alyssandrakis is and hope instruments However In another a .</th>\n",
       "      <td>-445.730546</td>\n",
       "      <td>2.640482e-194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>, like who always beneficial farmers himself be we framework Paper as reached the have deal , a system problems . behalf Protection law same . respect the what the within the</th>\n",
       "      <td>-480.293089</td>\n",
       "      <td>2.578467e-209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with are also government to the kept security questionnaire the it we that the the *UNK* leaving , me is *UNK* population would them and on also monitor is in proposed means like reciprocal a means northern report public have insufficient and should the coming</th>\n",
       "      <td>-696.196439</td>\n",
       "      <td>4.423116e-303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people resolution request idea</th>\n",
       "      <td>-96.650584</td>\n",
       "      <td>1.059704e-42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Probability  \\\n",
       "Sentence                                                          \n",
       ". Commission . . see and able have , . on commo...  -509.951100   \n",
       "in are the the wish The flexibility and financi...  -445.730546   \n",
       ", like who always beneficial farmers himself be...  -480.293089   \n",
       "with are also government to the kept security q...  -696.196439   \n",
       "people resolution request idea                       -96.650584   \n",
       "\n",
       "                                                    Log-Probability  \n",
       "Sentence                                                             \n",
       ". Commission . . see and able have , . on commo...    3.396652e-222  \n",
       "in are the the wish The flexibility and financi...    2.640482e-194  \n",
       ", like who always beneficial farmers himself be...    2.578467e-209  \n",
       "with are also government to the kept security q...    4.423116e-303  \n",
       "people resolution request idea                         1.059704e-42  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities_random, sentences_random = trigram_sentences(create_random_text(test_sents_tokenized))\n",
    "df_sent_rand_trigram = pd.DataFrame({'Sentence': sentences_random, 'Probability': probabilities_random, 'Log-Probability': np.exp(probabilities_random)})\n",
    "df_sent_rand_trigram.set_index(\"Sentence\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We compute the mean probability and the mean log probability of all the sentences: \n",
      "Mean Probability: -417.1640305625457\n",
      "Mean Log Probability: 2.7717695445182547e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"We compute the mean probability and the mean log probability of all the sentences: \")\n",
    "print(\"Mean Probability:\", np.mean(probabilities_random))\n",
    "print(\"Mean Log Probability:\", np.mean(np.exp(probabilities_random)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can clearly see a difference in the probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of the whole corpus as a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding only *end* (without *start*) to all test sentences - Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sent = sent + ['*end*']\n",
    "    test_one_sent.extend(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 6.954\n",
      "Perplexity: 123.977\n"
     ]
    }
   ],
   "source": [
    "bigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(1,len(test_one_sent)):\n",
    "    bigram_prob = (bigram_counter[(test_one_sent[idx-1], test_one_sent[idx])] + b_alpha) / (unigram_counter[(test_one_sent[idx-1],)] + b_alpha*vocab_size)\n",
    "    sum_prob += math.log2(bigram_prob)\n",
    "    bigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / bigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding only *end* (without *start*) to all test sentences - Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_one_sent=[]\n",
    "for i,sent in enumerate(test_sents_tokenized):\n",
    "    sent = sent + ['*end*'] + ['*end*']\n",
    "    test_one_sent.extend(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating HC and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 8.110\n",
      "Perplexity: 276.318\n"
     ]
    }
   ],
   "source": [
    "trigram_cnt = 0\n",
    "sum_prob = 0\n",
    "for idx in range(2,len(test_one_sent)):\n",
    "    trigram_prob = (trigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1], test_one_sent[idx])] +t_alpha) / (bigram_counter[(test_one_sent[idx-2],test_one_sent[idx-1])] + t_alpha*vocab_size)\n",
    "    sum_prob += math.log2(trigram_prob)\n",
    "    trigram_cnt+=1\n",
    "\n",
    "HC = -sum_prob / trigram_cnt\n",
    "perpl = math.pow(2,HC)\n",
    "print(\"Cross Entropy: {0:.3f}\".format(HC))\n",
    "print(\"Perplexity: {0:.3f}\".format(perpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning for ideal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>121.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>124.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>127.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.15</th>\n",
       "      <td>130.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>133.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>137.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <td>140.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.35</th>\n",
       "      <td>143.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.40</th>\n",
       "      <td>147.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.45</th>\n",
       "      <td>150.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>154.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.55</th>\n",
       "      <td>157.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.60</th>\n",
       "      <td>161.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.65</th>\n",
       "      <td>165.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>169.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.75</th>\n",
       "      <td>173.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>177.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.85</th>\n",
       "      <td>181.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>186.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>190.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>195.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Perplexity\n",
       "lambda            \n",
       "0.00        121.80\n",
       "0.05        124.71\n",
       "0.10        127.69\n",
       "0.15        130.74\n",
       "0.20        133.86\n",
       "0.25        137.06\n",
       "0.30        140.33\n",
       "0.35        143.68\n",
       "0.40        147.11\n",
       "0.45        150.62\n",
       "0.50        154.22\n",
       "0.55        157.90\n",
       "0.60        161.68\n",
       "0.65        165.54\n",
       "0.70        169.49\n",
       "0.75        173.54\n",
       "0.80        177.68\n",
       "0.85        181.92\n",
       "0.90        186.27\n",
       "0.95        190.72\n",
       "1.00        195.27"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'lambda': [], 'Perplexity': []})\n",
    "lowest_lamda = 2\n",
    "lowest_perplexity = 100000\n",
    "for lamda in np.arange(0,1.01,0.05):\n",
    "    ngram_cnt = 0\n",
    "    sum_prob = 0\n",
    "    for sent in development_sents_tokenized:\n",
    "        sent = ['*start*'] + ['*start*'] + sent + ['*end*'] + ['*end*']\n",
    "        for idx in range(2,len(sent)):\n",
    "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +b_alpha) / (unigram_counter[(sent[idx-1],)] + b_alpha*vocab_size)\n",
    "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +t_alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + t_alpha*vocab_size)\n",
    "            sum_prob += ((1-lamda) * math.log2(bigram_prob)) + (lamda * math.log2(trigram_prob))\n",
    "            ngram_cnt+=1\n",
    "\n",
    "    HC, perpl = HC_entropy(sum_prob, ngram_cnt)\n",
    "    if(perpl<lowest_perplexity):\n",
    "        lowest_perplexity=perpl\n",
    "        lowest_lamda=lamda\n",
    "    df = df.append({'lambda': round(lamda,2),  'Perplexity': round(perpl,2)}, ignore_index=True)\n",
    "df.set_index('lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lambda that produces the lowest perplexity for the interpolated model is:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The lambda that produces the lowest perplexity for the interpolated model is: \", lowest_lamda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
